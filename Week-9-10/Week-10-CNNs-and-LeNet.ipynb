{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "We will now build toward the next big idea in neural networks: convolutional neural networks (frequently called CNNs or ConvNets). These are nets that take some inspiration from the way neurons are connected in an animal's visual cortex, where different groups of neurons respond to specific portions of the visual field and then those signals are combined in a sort of hierarchy that is thought to allow animals to extract increasingly complex features within the incoming visual stimuli.\n",
    "\n",
    "CNNs are are feedforward neural networks like we have seen, but neurons in one layer are not necessarily connected to every neuron in the next layer. Such a more complex architecture, in some sense, allows a regularized version of fully-connected nets in that, they can generalize better to test data and to the real world. Recall, we could learn the whole training set for CIFAR-10 but had at least a 30% less accuracy on test data. Another benefit is that this more sparse sort of structure means we can build deeper or wider neural nets without growing the computation as much as fully-connected nets.\n",
    "\n",
    "CNNs are some of the best algorithms for image and video recognition problems, but have also been used effectively in anomaly detection, time series analysis in financial markets, and predicting the interactions between proteins and molecules in drug discovery among other applications.\n",
    "\n",
    "(Please see the class notes for introductory material. We continue with implementing some CNNs.)\n",
    "\n",
    "As we have seen, implementing CNNs is not overly complex, as there is a version of backpropagation that will work and the feature extractors in convolutional layers work fairly simply. However, CNNs are generally pretty expensive to run, so in the interest of having optimized code, we will use Keras code for our CNNs in the class.\n",
    "\n",
    "## ShallowNet\n",
    "\n",
    "Let's make a small CNN called ShallowNet just to get a sense of how to use Keras to build one. First, import some things from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a class for the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNet:\n",
    "    # create the architecture\n",
    "    def build(height, width, depth, numFilters, classes):\n",
    "        # create a feedforward neural net\n",
    "        model = Sequential()\n",
    "        \n",
    "        # add a convolutional layer with numFilters number of 3x3 filters\n",
    "        model.add(Conv2D(numFilters, (3, 3), padding='same', input_shape = (height, width, depth)))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        # add a softmax classifier\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation('softmax'))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try to run it on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 30s 633us/sample - loss: 0.4778 - accuracy: 0.8722 - val_loss: 0.3005 - val_accuracy: 0.9149\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 29s 597us/sample - loss: 0.3100 - accuracy: 0.9105 - val_loss: 0.2815 - val_accuracy: 0.9202\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 29s 595us/sample - loss: 0.2827 - accuracy: 0.9194 - val_loss: 0.2637 - val_accuracy: 0.9250\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 28s 574us/sample - loss: 0.2555 - accuracy: 0.9280 - val_loss: 0.2292 - val_accuracy: 0.9367\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 28s 588us/sample - loss: 0.2256 - accuracy: 0.9373 - val_loss: 0.2018 - val_accuracy: 0.9463\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 29s 600us/sample - loss: 0.1967 - accuracy: 0.9444 - val_loss: 0.1799 - val_accuracy: 0.9516\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 30s 624us/sample - loss: 0.1699 - accuracy: 0.9521 - val_loss: 0.1624 - val_accuracy: 0.9575\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 30s 633us/sample - loss: 0.1485 - accuracy: 0.9582 - val_loss: 0.1412 - val_accuracy: 0.9640\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 30s 632us/sample - loss: 0.1308 - accuracy: 0.9639 - val_loss: 0.1276 - val_accuracy: 0.9662\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 30s 624us/sample - loss: 0.1176 - accuracy: 0.9676 - val_loss: 0.1229 - val_accuracy: 0.9678\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 31s 654us/sample - loss: 0.1072 - accuracy: 0.9715 - val_loss: 0.1129 - val_accuracy: 0.9698\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 30s 627us/sample - loss: 0.0982 - accuracy: 0.9733 - val_loss: 0.1094 - val_accuracy: 0.9719\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 33s 698us/sample - loss: 0.0911 - accuracy: 0.9756 - val_loss: 0.1016 - val_accuracy: 0.9729\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 29s 611us/sample - loss: 0.0853 - accuracy: 0.9770 - val_loss: 0.0991 - val_accuracy: 0.9740\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 28s 581us/sample - loss: 0.0800 - accuracy: 0.9786 - val_loss: 0.0942 - val_accuracy: 0.9752\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 28s 592us/sample - loss: 0.0756 - accuracy: 0.9793 - val_loss: 0.0918 - val_accuracy: 0.9757\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 28s 583us/sample - loss: 0.0716 - accuracy: 0.9806 - val_loss: 0.0892 - val_accuracy: 0.9763\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 3001s 63ms/sample - loss: 0.0683 - accuracy: 0.9814 - val_loss: 0.0872 - val_accuracy: 0.9769\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 19s 402us/sample - loss: 0.0653 - accuracy: 0.9821 - val_loss: 0.0856 - val_accuracy: 0.9765\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 17s 361us/sample - loss: 0.0622 - accuracy: 0.9830 - val_loss: 0.0840 - val_accuracy: 0.9774\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 17s 361us/sample - loss: 0.0597 - accuracy: 0.9836 - val_loss: 0.0852 - val_accuracy: 0.9767\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 16s 344us/sample - loss: 0.0573 - accuracy: 0.9843 - val_loss: 0.0822 - val_accuracy: 0.9772\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 16s 330us/sample - loss: 0.0551 - accuracy: 0.9848 - val_loss: 0.0811 - val_accuracy: 0.9780\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 15s 313us/sample - loss: 0.0528 - accuracy: 0.9856 - val_loss: 0.0807 - val_accuracy: 0.9779\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 15s 307us/sample - loss: 0.0514 - accuracy: 0.9859 - val_loss: 0.0797 - val_accuracy: 0.9775\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 15s 308us/sample - loss: 0.0494 - accuracy: 0.9864 - val_loss: 0.0776 - val_accuracy: 0.9784\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 15s 310us/sample - loss: 0.0478 - accuracy: 0.9870 - val_loss: 0.0773 - val_accuracy: 0.9781\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 15s 311us/sample - loss: 0.0461 - accuracy: 0.9875 - val_loss: 0.0775 - val_accuracy: 0.9789\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 16s 329us/sample - loss: 0.0446 - accuracy: 0.9876 - val_loss: 0.0771 - val_accuracy: 0.9787\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 15s 320us/sample - loss: 0.0432 - accuracy: 0.9883 - val_loss: 0.0750 - val_accuracy: 0.9794\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 15s 312us/sample - loss: 0.0418 - accuracy: 0.9889 - val_loss: 0.0747 - val_accuracy: 0.9800\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 15s 310us/sample - loss: 0.0409 - accuracy: 0.9891 - val_loss: 0.0747 - val_accuracy: 0.9794\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 15s 311us/sample - loss: 0.0396 - accuracy: 0.9895 - val_loss: 0.0742 - val_accuracy: 0.9795\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 16s 324us/sample - loss: 0.0383 - accuracy: 0.9901 - val_loss: 0.0739 - val_accuracy: 0.9794\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 16s 341us/sample - loss: 0.0373 - accuracy: 0.9903 - val_loss: 0.0752 - val_accuracy: 0.9783\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 15s 316us/sample - loss: 0.0362 - accuracy: 0.9906 - val_loss: 0.0745 - val_accuracy: 0.9793\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 15s 322us/sample - loss: 0.0351 - accuracy: 0.9908 - val_loss: 0.0728 - val_accuracy: 0.9800\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 16s 328us/sample - loss: 0.0341 - accuracy: 0.9911 - val_loss: 0.0746 - val_accuracy: 0.9797\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 16s 326us/sample - loss: 0.0334 - accuracy: 0.9910 - val_loss: 0.0733 - val_accuracy: 0.9803\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 15s 310us/sample - loss: 0.0324 - accuracy: 0.9916 - val_loss: 0.0730 - val_accuracy: 0.9794\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 16s 328us/sample - loss: 0.0314 - accuracy: 0.9924 - val_loss: 0.0737 - val_accuracy: 0.9798\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 16s 334us/sample - loss: 0.0307 - accuracy: 0.9926 - val_loss: 0.0730 - val_accuracy: 0.9800\n",
      "Epoch 43/100\n",
      "48000/48000 [==============================] - 17s 350us/sample - loss: 0.0299 - accuracy: 0.9925 - val_loss: 0.0737 - val_accuracy: 0.9797\n",
      "Epoch 44/100\n",
      "48000/48000 [==============================] - 15s 316us/sample - loss: 0.0294 - accuracy: 0.9926 - val_loss: 0.0731 - val_accuracy: 0.9798\n",
      "Epoch 45/100\n",
      "48000/48000 [==============================] - 16s 336us/sample - loss: 0.0285 - accuracy: 0.9930 - val_loss: 0.0733 - val_accuracy: 0.9797\n",
      "Epoch 46/100\n",
      "48000/48000 [==============================] - 16s 337us/sample - loss: 0.0278 - accuracy: 0.9934 - val_loss: 0.0737 - val_accuracy: 0.9797\n",
      "Epoch 47/100\n",
      "48000/48000 [==============================] - 16s 338us/sample - loss: 0.0272 - accuracy: 0.9937 - val_loss: 0.0730 - val_accuracy: 0.9802\n",
      "Epoch 48/100\n",
      "48000/48000 [==============================] - 16s 339us/sample - loss: 0.0264 - accuracy: 0.9939 - val_loss: 0.0732 - val_accuracy: 0.9803\n",
      "Epoch 49/100\n",
      "48000/48000 [==============================] - 16s 339us/sample - loss: 0.0259 - accuracy: 0.9940 - val_loss: 0.0760 - val_accuracy: 0.9793\n",
      "Epoch 50/100\n",
      "48000/48000 [==============================] - 16s 338us/sample - loss: 0.0253 - accuracy: 0.9945 - val_loss: 0.0731 - val_accuracy: 0.9798\n",
      "Epoch 51/100\n",
      "48000/48000 [==============================] - 16s 331us/sample - loss: 0.0247 - accuracy: 0.9942 - val_loss: 0.0739 - val_accuracy: 0.9799\n",
      "Epoch 52/100\n",
      "48000/48000 [==============================] - 15s 320us/sample - loss: 0.0241 - accuracy: 0.9948 - val_loss: 0.0744 - val_accuracy: 0.9797\n",
      "Epoch 53/100\n",
      "48000/48000 [==============================] - 15s 318us/sample - loss: 0.0235 - accuracy: 0.9947 - val_loss: 0.0749 - val_accuracy: 0.9796\n",
      "Epoch 54/100\n",
      "48000/48000 [==============================] - 16s 342us/sample - loss: 0.0227 - accuracy: 0.9952 - val_loss: 0.0756 - val_accuracy: 0.9787\n",
      "Epoch 55/100\n",
      "48000/48000 [==============================] - 18s 367us/sample - loss: 0.0223 - accuracy: 0.9950 - val_loss: 0.0751 - val_accuracy: 0.9797\n",
      "Epoch 56/100\n",
      "48000/48000 [==============================] - 18s 374us/sample - loss: 0.0218 - accuracy: 0.9953 - val_loss: 0.0740 - val_accuracy: 0.9803\n",
      "Epoch 57/100\n",
      "48000/48000 [==============================] - 18s 377us/sample - loss: 0.0214 - accuracy: 0.9955 - val_loss: 0.0749 - val_accuracy: 0.9800\n",
      "Epoch 58/100\n",
      "48000/48000 [==============================] - 18s 382us/sample - loss: 0.0207 - accuracy: 0.9955 - val_loss: 0.0741 - val_accuracy: 0.9797\n",
      "Epoch 59/100\n",
      "48000/48000 [==============================] - 17s 350us/sample - loss: 0.0204 - accuracy: 0.9957 - val_loss: 0.0751 - val_accuracy: 0.9805\n",
      "Epoch 60/100\n",
      "48000/48000 [==============================] - 18s 367us/sample - loss: 0.0198 - accuracy: 0.9956 - val_loss: 0.0767 - val_accuracy: 0.9799\n",
      "Epoch 61/100\n",
      "48000/48000 [==============================] - 19s 390us/sample - loss: 0.0194 - accuracy: 0.9960 - val_loss: 0.0762 - val_accuracy: 0.9798\n",
      "Epoch 62/100\n",
      "48000/48000 [==============================] - 18s 371us/sample - loss: 0.0191 - accuracy: 0.9960 - val_loss: 0.0754 - val_accuracy: 0.9795\n",
      "Epoch 63/100\n",
      "48000/48000 [==============================] - 21s 431us/sample - loss: 0.0185 - accuracy: 0.9964 - val_loss: 0.0760 - val_accuracy: 0.9798\n",
      "Epoch 64/100\n",
      "48000/48000 [==============================] - 19s 397us/sample - loss: 0.0182 - accuracy: 0.9965 - val_loss: 0.0768 - val_accuracy: 0.9794\n",
      "Epoch 65/100\n",
      "48000/48000 [==============================] - 18s 385us/sample - loss: 0.0179 - accuracy: 0.9964 - val_loss: 0.0748 - val_accuracy: 0.9800\n",
      "Epoch 66/100\n",
      "48000/48000 [==============================] - 17s 364us/sample - loss: 0.0175 - accuracy: 0.9965 - val_loss: 0.0751 - val_accuracy: 0.9803\n",
      "Epoch 67/100\n",
      "48000/48000 [==============================] - 19s 386us/sample - loss: 0.0172 - accuracy: 0.9969 - val_loss: 0.0753 - val_accuracy: 0.9803\n",
      "Epoch 68/100\n",
      "48000/48000 [==============================] - 19s 400us/sample - loss: 0.0166 - accuracy: 0.9972 - val_loss: 0.0765 - val_accuracy: 0.9799\n",
      "Epoch 69/100\n",
      "48000/48000 [==============================] - 17s 354us/sample - loss: 0.0165 - accuracy: 0.9968 - val_loss: 0.0778 - val_accuracy: 0.9801\n",
      "Epoch 70/100\n",
      "48000/48000 [==============================] - 17s 346us/sample - loss: 0.0162 - accuracy: 0.9972 - val_loss: 0.0768 - val_accuracy: 0.9793\n",
      "Epoch 71/100\n",
      "48000/48000 [==============================] - 17s 348us/sample - loss: 0.0158 - accuracy: 0.9971 - val_loss: 0.0762 - val_accuracy: 0.9802\n",
      "Epoch 72/100\n",
      "48000/48000 [==============================] - 17s 349us/sample - loss: 0.0155 - accuracy: 0.9970 - val_loss: 0.0776 - val_accuracy: 0.9795\n",
      "Epoch 73/100\n",
      "48000/48000 [==============================] - 17s 347us/sample - loss: 0.0152 - accuracy: 0.9974 - val_loss: 0.0774 - val_accuracy: 0.9793\n",
      "Epoch 74/100\n",
      "48000/48000 [==============================] - 17s 348us/sample - loss: 0.0149 - accuracy: 0.9974 - val_loss: 0.0787 - val_accuracy: 0.9790\n",
      "Epoch 75/100\n",
      "48000/48000 [==============================] - 17s 347us/sample - loss: 0.0145 - accuracy: 0.9973 - val_loss: 0.0792 - val_accuracy: 0.9790\n",
      "Epoch 76/100\n",
      "48000/48000 [==============================] - 17s 359us/sample - loss: 0.0143 - accuracy: 0.9975 - val_loss: 0.0781 - val_accuracy: 0.9795\n",
      "Epoch 77/100\n",
      "48000/48000 [==============================] - 17s 358us/sample - loss: 0.0140 - accuracy: 0.9975 - val_loss: 0.0775 - val_accuracy: 0.9803\n",
      "Epoch 78/100\n",
      "48000/48000 [==============================] - 18s 370us/sample - loss: 0.0138 - accuracy: 0.9977 - val_loss: 0.0778 - val_accuracy: 0.9797\n",
      "Epoch 79/100\n",
      "48000/48000 [==============================] - 17s 346us/sample - loss: 0.0135 - accuracy: 0.9978 - val_loss: 0.0788 - val_accuracy: 0.9798\n",
      "Epoch 80/100\n",
      "48000/48000 [==============================] - 17s 361us/sample - loss: 0.0131 - accuracy: 0.9981 - val_loss: 0.0793 - val_accuracy: 0.9799\n",
      "Epoch 81/100\n",
      "48000/48000 [==============================] - 17s 360us/sample - loss: 0.0129 - accuracy: 0.9983 - val_loss: 0.0790 - val_accuracy: 0.9794\n",
      "Epoch 82/100\n",
      "48000/48000 [==============================] - 18s 368us/sample - loss: 0.0126 - accuracy: 0.9982 - val_loss: 0.0800 - val_accuracy: 0.9791\n",
      "Epoch 83/100\n",
      "48000/48000 [==============================] - 17s 357us/sample - loss: 0.0125 - accuracy: 0.9981 - val_loss: 0.0807 - val_accuracy: 0.9795\n",
      "Epoch 84/100\n",
      "48000/48000 [==============================] - 17s 344us/sample - loss: 0.0123 - accuracy: 0.9981 - val_loss: 0.0799 - val_accuracy: 0.9797\n",
      "Epoch 85/100\n",
      "48000/48000 [==============================] - 18s 368us/sample - loss: 0.0121 - accuracy: 0.9982 - val_loss: 0.0799 - val_accuracy: 0.9798\n",
      "Epoch 86/100\n",
      "48000/48000 [==============================] - 19s 393us/sample - loss: 0.0118 - accuracy: 0.9983 - val_loss: 0.0807 - val_accuracy: 0.9796\n",
      "Epoch 87/100\n",
      "48000/48000 [==============================] - 16s 339us/sample - loss: 0.0116 - accuracy: 0.9984 - val_loss: 0.0800 - val_accuracy: 0.9797\n",
      "Epoch 88/100\n",
      "48000/48000 [==============================] - 17s 362us/sample - loss: 0.0113 - accuracy: 0.9984 - val_loss: 0.0813 - val_accuracy: 0.9797\n",
      "Epoch 89/100\n",
      "48000/48000 [==============================] - 16s 337us/sample - loss: 0.0112 - accuracy: 0.9984 - val_loss: 0.0813 - val_accuracy: 0.9796\n",
      "Epoch 90/100\n",
      "48000/48000 [==============================] - 16s 337us/sample - loss: 0.0110 - accuracy: 0.9987 - val_loss: 0.0801 - val_accuracy: 0.9793\n",
      "Epoch 91/100\n",
      "48000/48000 [==============================] - 17s 353us/sample - loss: 0.0107 - accuracy: 0.9986 - val_loss: 0.0810 - val_accuracy: 0.9799\n",
      "Epoch 92/100\n",
      "48000/48000 [==============================] - 18s 381us/sample - loss: 0.0105 - accuracy: 0.9987 - val_loss: 0.0812 - val_accuracy: 0.9795\n",
      "Epoch 93/100\n",
      "48000/48000 [==============================] - 18s 366us/sample - loss: 0.0105 - accuracy: 0.9986 - val_loss: 0.0815 - val_accuracy: 0.9795\n",
      "Epoch 94/100\n",
      "48000/48000 [==============================] - 17s 358us/sample - loss: 0.0103 - accuracy: 0.9989 - val_loss: 0.0819 - val_accuracy: 0.9797\n",
      "Epoch 95/100\n",
      "48000/48000 [==============================] - 17s 348us/sample - loss: 0.0100 - accuracy: 0.9988 - val_loss: 0.0820 - val_accuracy: 0.9791\n",
      "Epoch 96/100\n",
      "48000/48000 [==============================] - 17s 356us/sample - loss: 0.0099 - accuracy: 0.9989 - val_loss: 0.0826 - val_accuracy: 0.9790\n",
      "Epoch 97/100\n",
      "48000/48000 [==============================] - 17s 357us/sample - loss: 0.0096 - accuracy: 0.9990 - val_loss: 0.0822 - val_accuracy: 0.9796\n",
      "Epoch 98/100\n",
      "48000/48000 [==============================] - 17s 352us/sample - loss: 0.0094 - accuracy: 0.9989 - val_loss: 0.0823 - val_accuracy: 0.9797\n",
      "Epoch 99/100\n",
      "48000/48000 [==============================] - 17s 357us/sample - loss: 0.0094 - accuracy: 0.9989 - val_loss: 0.0831 - val_accuracy: 0.9799\n",
      "Epoch 100/100\n",
      "48000/48000 [==============================] - 16s 341us/sample - loss: 0.0093 - accuracy: 0.9989 - val_loss: 0.0836 - val_accuracy: 0.9794\n",
      "\n",
      " Test accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.98      0.98      0.98      1032\n",
      "           3       0.98      0.99      0.98      1010\n",
      "           4       0.98      0.99      0.99       982\n",
      "           5       0.98      0.98      0.98       892\n",
      "           6       0.99      0.98      0.98       958\n",
      "           7       0.97      0.98      0.98      1028\n",
      "           8       0.98      0.97      0.98       974\n",
      "           9       0.99      0.96      0.97      1009\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
    "\n",
    "trainX = trainX.astype('float32')/255.0\n",
    "testX = testX.astype('float32')/255.0\n",
    "\n",
    "trainX = np.expand_dims(trainX, -1)\n",
    "testX = np.expand_dims(testX, -1)\n",
    "\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)\n",
    "\n",
    "model = ShallowNet.build(28, 28, 1, 32, 10)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = SGD(0.01), metrics = ['accuracy'])\n",
    "\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit(trainX, trainY, validation_split = 0.20, batch_size = 32, epochs = 100, verbose = 1)\n",
    "\n",
    "print('\\n Test accuracy')\n",
    "predictedY = model.predict(testX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "testY = testY.argmax(axis=1)\n",
    "print(classification_report(testY, predictedY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
    "\n",
    "trainX = trainX.astype('float32')/255.0\n",
    "testX = testX.astype('float32')/255.0\n",
    "\n",
    "trainX = np.expand_dims(trainX, -1)\n",
    "testX = np.expand_dims(testX, -1)\n",
    "\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)\n",
    "\n",
    "model = ShallowNet.build(28, 28, 1, 32, 10)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit(trainX, trainY, validation_split = 0.20, batch_size = 32, epochs = 100, verbose = 1)\n",
    "\n",
    "print('\\n Test accuracy')\n",
    "predictedY = model.predict(testX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "testY = testY.argmax(axis=1)\n",
    "print(classification_report(testY, predictedY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "class LeNet:\n",
    "    # create the architecture\n",
    "    def build(height, width, depth, classes):\n",
    "        # create a feedforward neural net\n",
    "        model = Sequential()\n",
    "        \n",
    "        # add a convolutional layer with 20 5x5 filters and a 2x2 max pooling layer\n",
    "        model.add(Conv2D(20, (5, 5), padding = 'same', input_shape = (height, width, depth), activation = 'tanh'))\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "        \n",
    "        # add another convolutional layer with 50 5x5 filters and a 2x2 max pooling layer\n",
    "        model.add(Conv2D(50, (5, 5), padding='same'))\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "        \n",
    "        # add a fully-connected layer\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(500, activation = 'tanh'))\n",
    "        \n",
    "        # add a softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation('softmax'))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the model on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 45s 937us/sample - loss: 0.1791 - accuracy: 0.9455 - val_loss: 0.0649 - val_accuracy: 0.9805\n",
      "Epoch 2/20\n",
      "28672/48000 [================>.............] - ETA: 17s - loss: 0.0574 - accuracy: 0.9823"
     ]
    }
   ],
   "source": [
    "((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
    "\n",
    "trainX = trainX.astype('float32')/255.0\n",
    "testX = testX.astype('float32')/255.0\n",
    "\n",
    "trainX = np.expand_dims(trainX, -1)\n",
    "testX = np.expand_dims(testX, -1)\n",
    "\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)\n",
    "\n",
    "model = LeNet.build(28, 28, 1, 10)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit(trainX, trainY, validation_split = 0.20, batch_size = 128, epochs = 20, verbose = 1)\n",
    "\n",
    "print('\\n Test accuracy')\n",
    "predictedY = model.predict(testX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "testY = testY.argmax(axis=1)\n",
    "print(classification_report(testY, predictedY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
