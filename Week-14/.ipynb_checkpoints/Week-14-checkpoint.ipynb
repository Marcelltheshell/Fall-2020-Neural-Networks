{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Recurrent neural networks are simply networks that can have connections going backward in the network, unlike the nets we have seen so far.\n",
    "\n",
    "Here, we introduce an idea of timesteps where we input a datapoint to the net at each timestep. As a datapoint passes through an RNN, it will proceed to the output layer but the information being sent through the reverse connections will be fed into the net while the next datapoint is passed through the RNN.\n",
    "\n",
    "## Why Are Backward Connections Useful?\n",
    "\n",
    "The way RNNs operate means the current datapoint will receive information derived from the previous datapoint since weights will persist and feed information into the net as a new datapoint goes through the net.\n",
    "\n",
    "The prior information continues to influence how the net operates because, at a third timestep, the information will continue going through the loops in the network architecture, the same at the fourth step, and so on. Therefore, a fundamental difference between RNNs and the nets we have seen in the past is this persistent \"memory\" of previous datapoints, which allows us to use context of the previous datapoints to make inferences about each datapoint.\n",
    "\n",
    "### Examples\n",
    "\n",
    "If we have a paragraph of text, we could feed it into the net word-by-word and try to classify the whole sequence. It gives us the power of using the *context* built by the other words in the paragraph. Any use of language, such as recognizing spoken words, handwritten text, or sign language, can benefit from this.\n",
    "\n",
    "In addition, in handwriting recognition where we have a whole word of text, certainly knowing some letters in the word can help you figure out an unknown letter in the center. For example, if we have a K, it is very very unlikely the next letter will be a Q in English. For another example, here's a word I wrote:\n",
    "\n",
    "![img](feature.png)\n",
    "\n",
    "Obviously, being an intelligent human, you know the third letter is an 'a' and not a 'u' because \"feuture\" isn't a word and your professor has passable spelling skills. However, looking at that letter in isolation makes its identity totally ambiguous. Letter/digit recognition is the kind of task we have assigned to neural nets before, but RNNs have a more unique ability to read letters one-by-one, and use the past letters to influence later classifications.\n",
    "\n",
    "The commonality in all of these examples is that they all involve making inferences about *sequences* of inputs (letters, words, signs), rather than just individual inputs.\n",
    "\n",
    "Note that we are talking only of new datapoints being influenced by prior datapoints, but we will also learn about bidirectional RNNs, which look both forward and backward in time by distances the net will learn.\n",
    "\n",
    "Of course, CNNs also use local structure within datapoints to make inferences about the whole datapoint too, but we have to specify what size filters and how many filters to use for this. With RNNs, the net will automatically learn to use as much of the past information as it needs to use for a given point.\n",
    "\n",
    "## Training RNNs\n",
    "\n",
    "The most common approach to training RNNs is essentially the same as any other neural net we have seen: stochastic gradient descent and backpropagation. SGD will operate just the same as the other nets. Recall we have used backpropagation to compute exact gradients needed by SGD by systematic use of the chain rule propagating backwards from the loss function to the weights and biases in the network.\n",
    "\n",
    "RNNs present a challenge to this idea--what does it mean to propagate backwards in a net that has loops?! There would be infinite paths the method could take since there are loops it could traverse arbitrarily many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "One area of application of RNNs is in sentiment analysis--attempts to identify the feelings associated with written text. Examples:\n",
    "\n",
    "* Is a review of a product or service or song or movie positive, neutral, or negative?\n",
    "* Was someone happy or sad in describing their day?\n",
    "* Are song lyrics sad or happy or excited?\n",
    "\n",
    "### Movie Reviews\n",
    "\n",
    "Let's see what we can do with movie reviews for an experiment in sentiment analysis. The data comes from the Internet Movie Database (IMBD), provided by\n",
    "\n",
    "* Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). [Learning Word Vectors for Sentiment Analysis](https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf). *The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)*.\n",
    "\n",
    "The dataset contains 50,000 reviews (half in a training set, half in a testing set). Each dataset has reviews from disjoint sets of movies, at most 30 per movie. Each review on IMDB includes a rating from 0 to 10, but the data includes only negative reviews ($\\leq 4$ rating) and positive reviews ($\\geq 6$ rating) as the labels for the datapoints.\n",
    "\n",
    "We will use the dataset to try to classify the reviews as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "from tensorflow.strings import regex_replace\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the imdb dataset\n",
    "with zipfile.ZipFile('../datasets/imdb.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('../datasets/imdb/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# clean the data by removing linebreaks\n",
    "def prepareData(dir):\n",
    "    # read the directory of datapoints and labels into a Dataset object\n",
    "    data = text_dataset_from_directory(dir)\n",
    "    \n",
    "    # replace HTML linebreaks from the text with spaces\n",
    "    return data.map(lambda text, label: (regex_replace(text, '<br />', ' '), label))\n",
    "\n",
    "# read the directory into memory and clean the text\n",
    "trainData = prepareData('../datasets/imdb/train')\n",
    "testData = prepareData('../datasets/imdb/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'I have watched some pretty poor films in the past, but what the hell were they thinking of when they made this movie. Had the production crew turned into zombies when they came up with the idea of making it, because you sure have to be brain dead to find any enjoyment in it.  I am a fan of most genres and enjoy \"shoot \\'em up\" games, but merging the daft scenes from the game just made this ridiculous and unwatchable.  As most have already said, there was hardly any script and the acting was weak. I won\\'t waste my time describing it.  Anyone who rates this film above 4 has to be part of the production company or Sega, or else they have a very warped concept of entertainment.  I must say, I was more annoyed with the video shop, who gave this a thumbs up, which led me to rent it. Thank god I had a second film to watch to restore some of my faith in movies.  Comic book guy would be right if he said \"Worst movie ever\"!'\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# randomly print a review and label\n",
    "for text_batch, label_batch in trainData.take(1):\n",
    "    print(text_batch.numpy()[0])\n",
    "    print(label_batch.numpy()[0]) # 0 = negative, 1 = positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Net Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a TextVectorization layer to turn input string into a sequence of integers,\n",
    "# each representing one token\n",
    "maxTokens = 1000\n",
    "vectorizeLayer = TextVectorization(max_tokens = maxTokens,\n",
    "                                   output_mode = 'int',\n",
    "                                   output_sequence_length = 100)\n",
    "\n",
    "# adapt() fits the TextVectorization layer to our text dataset. This is when the\n",
    "# max_tokens most common words (i.e. the vocabulary) are selected.\n",
    "trainText = trainData.map(lambda text, label: text)\n",
    "\n",
    "vectorizeLayer.adapt(trainText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(1,), dtype = 'string'))\n",
    "\n",
    "# add layer to the model\n",
    "model.add(vectorizeLayer)\n",
    "\n",
    "# add an embedding layer to turn integers into fixed-length vectors\n",
    "model.add(Embedding(maxTokens + 1, 128))\n",
    "\n",
    "# add a fully-connected recurrent layer\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "\n",
    "# add a dense layer\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "\n",
    "# add softmax classifier\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization_2 (TextVe (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 100, 128)          128128    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100, 64)           8256      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100, 64)           4160      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100, 1)            65        \n",
      "=================================================================\n",
      "Total params: 140,609\n",
      "Trainable params: 140,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 21s 26ms/step - loss: 0.6882 - accuracy: 0.5288\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.6853 - accuracy: 0.5380\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 20s 25ms/step - loss: 0.6852 - accuracy: 0.5391\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.6851 - accuracy: 0.5390\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.6850 - accuracy: 0.5384\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 21s 26ms/step - loss: 0.6851 - accuracy: 0.5390\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.6850 - accuracy: 0.5388\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.6851 - accuracy: 0.5387\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.6850 - accuracy: 0.5388\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.6850 - accuracy: 0.5388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23d314d41c8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainData, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(1,), dtype = 'string'))\n",
    "\n",
    "# add layer to the model\n",
    "model.add(vectorizeLayer)\n",
    "\n",
    "# add an embedding layer to turn integers into fixed-length vectors\n",
    "model.add(Embedding(maxTokens + 1, 128))\n",
    "\n",
    "# add a fully-connected recurrent layer\n",
    "model.add(SimpleRNN(64))\n",
    "\n",
    "# add a dense layer\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "\n",
    "# add softmax classifier\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization_1 (TextVe (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 100, 128)          128128    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 64)                12352     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 144,705\n",
      "Trainable params: 144,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 25s 31ms/step - loss: 0.6958 - accuracy: 0.5152\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.6991 - accuracy: 0.5129\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.6929 - accuracy: 0.52690s - loss: 0.6930 - \n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.6666 - accuracy: 0.5967\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.6580 - accuracy: 0.6016\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.6900 - accuracy: 0.54190s - loss: 0.6900 - accuracy: 0.54\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 25s 31ms/step - loss: 0.6770 - accuracy: 0.5726\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.6804 - accuracy: 0.5572\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 25s 31ms/step - loss: 0.6833 - accuracy: 0.5601\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 25s 31ms/step - loss: 0.6822 - accuracy: 0.5465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23d2be48c08>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainData, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment With LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(1,), dtype = 'string'))\n",
    "\n",
    "# add layer to the model\n",
    "model.add(vectorizeLayer)\n",
    "\n",
    "# add an embedding layer to turn integers into fixed-length vectors\n",
    "model.add(Embedding(maxTokens + 1, 128))\n",
    "\n",
    "# add a fully-connected recurrent layer\n",
    "model.add(LSTM(64))\n",
    "\n",
    "# add a dense layer\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "\n",
    "# add softmax classifier\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 128)          128128    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 181,761\n",
      "Trainable params: 181,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 38s 47ms/step - loss: 0.5920 - accuracy: 0.6608\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 38s 48ms/step - loss: 0.4502 - accuracy: 0.7941\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 37s 48ms/step - loss: 0.4238 - accuracy: 0.8044\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 37s 48ms/step - loss: 0.4056 - accuracy: 0.82010s - loss: 0.4058 - accura\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 37s 48ms/step - loss: 0.3790 - accuracy: 0.8311\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 38s 48ms/step - loss: 0.3691 - accuracy: 0.8394\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 38s 48ms/step - loss: 0.3468 - accuracy: 0.8511\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 38s 48ms/step - loss: 0.3355 - accuracy: 0.8551\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 38s 48ms/step - loss: 0.3184 - accuracy: 0.8666\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 38s 48ms/step - loss: 0.3149 - accuracy: 0.8639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23d2b9bd088>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainData, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the unzipped imdb dataset (this is just so I can upload to GitHub efficiently)\n",
    "if os.path.isfile('../datasets/imdb/README'):\n",
    "    shutil.rmtree('../datasets/imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
