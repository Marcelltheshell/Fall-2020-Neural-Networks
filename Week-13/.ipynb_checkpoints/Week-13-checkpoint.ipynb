{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Improved Training\n",
    "\n",
    "This week, we investigate some useful methods for improving the performance of neural nets by ensemble methods, where we train several nets on the same datapoints and allow them to \"vote\" on the correct classification or regression prediction. Another helpful approach is to monitor training carefully. Lastly, we will speeding up training with transfer learning, where we start with pre-trained models and try to specialize them for our datasets.\n",
    "\n",
    "## Ensemble Methods\n",
    "\n",
    "A common technique used with classification methods (both non-neural and neural) is to take an ensemble of models and combine them to make classification decisions. For example, we could run 5 neural nets, each with comparable accuracy overall, and classify each datapoint based on the majority vote of the 5 networks.\n",
    "\n",
    "This almost always improves results compared to using just one net because different nets have unique talents and may make errors on different datapoints, but, assuming all the nets have good accuracy, they are typically correct, so these mistakes are frequently restricted to just a minority of models.\n",
    "\n",
    "Let's bring in a few (mini) modern CNN achitectures we wrote in the past to use for some ensembling. Note that any architecture would work for the forthcoming experiments, but the following nets can run relatively quickly.\n",
    "\n",
    "First, we import some things we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages\n",
    "from imutils import paths\n",
    "import glob\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import progressbar\n",
    "import random\n",
    "\n",
    "# sklearn functions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# keras functions\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniVGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create a class for a mini version of VGGNet (Simonyan and Zisserman, 2015)\n",
    "class MiniVGGNet:\n",
    "    def build(height, width, depth, classes):\n",
    "        # create the model and name it MiniVGGNet\n",
    "        model = Sequential(name = 'MiniVGGNet')\n",
    "                \n",
    "        # convolutional layer with 32 3x3 feature maps\n",
    "        model.add(Conv2D(32, (3, 3), padding = 'same', input_shape = (height, width, depth)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # convolutional layer with 32 3x3 feature maps\n",
    "        model.add(Conv2D(32, (3, 3), padding = 'same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # 2x2 max pooling layer with stride 2x2\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        # convolutional layer with 64 3x3 feature maps\n",
    "        model.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # convolutional layer with 64 3x3 feature maps\n",
    "        model.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # 2x2 max pooling layer with stride 2x2\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        # flatten the activations from a square to a vector\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        # fully-connected layer\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        # fully-connected layer with softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation('softmax'))\n",
    "        \n",
    "        # return the model\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniGoogLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class MiniGoogLeNet:\n",
    "    def convolution_module(x, K, kX, kY, stride, channelsDim, padding=\"same\"):\n",
    "        # create a CONV -> BN -> RELU sequence\n",
    "        x = Conv2D(K, (kX, kY), strides = stride, padding = padding)(x)\n",
    "        x = BatchNormalization(axis = channelsDim)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        \n",
    "        # return the output\n",
    "        return x\n",
    "    \n",
    "    def inception_module(x, numberOf1x1Kernels, numberOf3x3Kernels, channelsDim):\n",
    "        # define two \"parallel\" convolutions of size 1x1 and 3x3 concatenated across the channels dimension\n",
    "        convolution_1x1 = MiniGoogLeNet.convolution_module(x, numberOf1x1Kernels, 1, 1, (1, 1), channelsDim)\n",
    "        convolution_3x3 = MiniGoogLeNet.convolution_module(x, numberOf3x3Kernels, 3, 3, (1, 1), channelsDim)\n",
    "        x = concatenate([convolution_1x1, convolution_3x3], axis = channelsDim)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def downsample_module(x, K, channelsDim):\n",
    "        # define a CONV and POOL and then concatenate across the channels dimension\n",
    "        convolution_3x3 = MiniGoogLeNet.convolution_module(x, K, 3, 3, (2, 2), channelsDim, padding = 'valid')\n",
    "        pool = MaxPooling2D((3, 3), strides = (2, 2))(x)\n",
    "        x = concatenate([convolution_3x3, pool], axis = channelsDim)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def build(width, height, depth, classes):\n",
    "        inputShape = (height, width, depth)\n",
    "        channelsDim = -1\n",
    "        \n",
    "        if backend.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            channelsDim = 1\n",
    "        \n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape = inputShape)\n",
    "        x = MiniGoogLeNet.convolution_module(inputs, 96, 3, 3, (1, 1), channelsDim)\n",
    "        \n",
    "        # two inception modules followed by a downsample module\n",
    "        x = MiniGoogLeNet.inception_module(x, 32, 32, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 32, 48, channelsDim)\n",
    "        x = MiniGoogLeNet.downsample_module(x, 80, channelsDim)\n",
    "        \n",
    "        # four inception modules followed by a downsample module\n",
    "        x = MiniGoogLeNet.inception_module(x, 112, 48, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 96, 64, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 80, 80, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 48, 96, channelsDim)\n",
    "        x = MiniGoogLeNet.downsample_module(x, 96, channelsDim)\n",
    "        \n",
    "        # two inception modules followed by global POOL and dropout\n",
    "        x = MiniGoogLeNet.inception_module(x, 176, 160, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 176, 160, channelsDim)\n",
    "        x = AveragePooling2D((7, 7))(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        \n",
    "        # softmax classifier\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(classes)(x)\n",
    "        x = Activation('softmax')(x)\n",
    "        \n",
    "        # create a model\n",
    "        model = Model(inputs, x, name='MiniGoogLeNet')\n",
    "        \n",
    "        # return the model\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ResNet:\n",
    "    def residual_module(data, K, stride, channelsDim, reduce = False, reg = 0.0001, bnEpsilon = 0.00002, bnMomentum = 0.9):\n",
    "        shortcut = data\n",
    "        \n",
    "        # 1x1 CONVs\n",
    "        bn1 = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(data)\n",
    "        act1 = Activation('relu')(bn1)\n",
    "        conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias = False, kernel_regularizer = l2(reg))(act1)\n",
    "        \n",
    "        # 3x3 CONVs\n",
    "        bn2 = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(conv1)\n",
    "        act2 = Activation('relu')(bn2)\n",
    "        conv2 = Conv2D(int(K * 0.25), (3, 3), strides = stride, padding = 'same', use_bias = False, kernel_regularizer = l2(reg))(act2)\n",
    "        \n",
    "        # 1x1 CONVs\n",
    "        bn3 = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(conv2)\n",
    "        act3 = Activation('relu')(bn3)\n",
    "        conv3 = Conv2D(K, (1, 1), use_bias = False, kernel_regularizer = l2(reg))(act3)\n",
    "        \n",
    "        # if we reduce the spatial size, apply a CONV layer to the shortcut\n",
    "        if reduce:\n",
    "            shortcut = Conv2D(K, (1, 1), strides = stride, use_bias = False, kernel_regularizer = l2(reg))(act1)\n",
    "            \n",
    "        # add the shortcut and the final CONV\n",
    "        x = add([conv3, shortcut])\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def build(width, height, depth, classes, stages, filters, reg = 0.0001, bnEpsilon = 0.00002, bnMomentum = 0.9, dataset='cifar'):\n",
    "        inputShape = (height, width, depth)\n",
    "        channelsDim = -1\n",
    "        \n",
    "        if backend.image_data_format() == 'channels_first':\n",
    "            inputShape = (depth, height, width)\n",
    "            channelsDim = 1\n",
    "            \n",
    "        # set the input and apply BN\n",
    "        inputs = Input(shape = inputShape)\n",
    "        x = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(inputs)\n",
    "        \n",
    "        if dataset == 'cifar':\n",
    "            # apply a single CONV layer\n",
    "            x = Conv2D(filters[0], (3, 3), use_bias = False, padding = 'same',\n",
    "                       kernel_regularizer = l2(reg))(x)\n",
    "        \n",
    "        # loop over the number of stages\n",
    "        for counter in range(0, len(stages)):\n",
    "            # initialize the stride\n",
    "            if counter == 0:\n",
    "                stride = (1, 1)\n",
    "            else:\n",
    "                stride = (2, 2)\n",
    "                    \n",
    "            # apply a residual module to reduce the spatial dimension of the image volume\n",
    "            x = ResNet.residual_module(x, filters[counter + 1], stride, channelsDim, reduce = True, bnEpsilon = bnEpsilon, bnMomentum = bnMomentum)\n",
    "            \n",
    "            # loop over the number of layers in the current stage\n",
    "            for j in range(0, stages[counter] - 1):\n",
    "                # apply a residual module\n",
    "                x = ResNet.residual_module(x, filters[counter + 1], (1, 1), channelsDim, bnEpsilon = bnEpsilon, bnMomentum = bnMomentum)\n",
    "                    \n",
    "        # apply BN -> ACT -> POOL\n",
    "        x = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = AveragePooling2D((8, 8))(x)\n",
    "        \n",
    "        # softmax classifier\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(classes, kernel_regularizer = l2(reg))(x)\n",
    "        x = Activation('softmax')(x)\n",
    "        \n",
    "        # create the model\n",
    "        model = Model(inputs, x, name = 'ResNet')\n",
    "        \n",
    "        # return the model\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Ensembles\n",
    "\n",
    "Now, let's look at some ensembling methods. In the simplest case, we train several nets and average the classifications at the end. If the nets have similar performance, but make mistakes on *different* examples, this approach improves performance in most cases.\n",
    "\n",
    "**Quick GPU Check**: Before we start training models, let's check our GPU resources. If you have a GPU set up to work with TensorFlow, its name will be output and it will be used in training. If not, none will be output and your training will use your CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "/device:GPU:0\n",
      "device: 0, name: GeForce RTX 2070, pci bus id: 0000:04:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "numGPUs = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "\n",
    "print(\"Num GPUs Available: \", numGPUs)\n",
    "\n",
    "if numGPUs > 0:\n",
    "    print(tf.test.gpu_device_name())\n",
    "    print(device_lib.list_local_devices()[1].physical_device_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with the Same Hyperparameters Multiple Times\n",
    "\n",
    "Next, let's train several MiniVGGNets on CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# load cifar10 data\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "labelNames = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", 'dog', \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "# preprocess data\n",
    "trainX = trainX.astype('float')/255.0\n",
    "testX = testX.astype('float')/255.0\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)\n",
    "\n",
    "# create an image generator for data augmentation with random shifting, rotation, and horizontal flips\n",
    "aug = ImageDataGenerator(rotation_range = 10, width_shift_range = 0.1, height_shift_range = 0.1, horizontal_flip = True, fill_mode = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net 0 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "781/781 [==============================] - 84s 107ms/step - loss: 1.6116 - accuracy: 0.4564 - val_loss: 1.2383 - val_accuracy: 0.5816\n",
      "Epoch 2/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 1.1182 - accuracy: 0.6013 - val_loss: 1.0832 - val_accuracy: 0.6394\n",
      "Epoch 3/10\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 0.9926 - accuracy: 0.6504 - val_loss: 1.1733 - val_accuracy: 0.6197\n",
      "Epoch 4/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.9169 - accuracy: 0.6774 - val_loss: 0.9389 - val_accuracy: 0.6803\n",
      "Epoch 5/10\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 0.8591 - accuracy: 0.7008 - val_loss: 0.9506 - val_accuracy: 0.6894\n",
      "Epoch 6/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.8172 - accuracy: 0.7133 - val_loss: 0.9262 - val_accuracy: 0.7068\n",
      "Epoch 7/10\n",
      "781/781 [==============================] - 23s 30ms/step - loss: 0.7842 - accuracy: 0.7266 - val_loss: 0.7594 - val_accuracy: 0.7410\n",
      "Epoch 8/10\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.7524 - accuracy: 0.7370 - val_loss: 0.7740 - val_accuracy: 0.7374\n",
      "Epoch 9/10\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.7269 - accuracy: 0.7465 - val_loss: 0.6835 - val_accuracy: 0.7693\n",
      "Epoch 10/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.7019 - accuracy: 0.7543 - val_loss: 0.6582 - val_accuracy: 0.7753\n",
      "WARNING:tensorflow:From C:\\Users\\Ryan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models1\\model_0.model\\assets\n",
      "Net 1 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 1.6192 - accuracy: 0.4509 - val_loss: 1.3308 - val_accuracy: 0.5245\n",
      "Epoch 2/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 1.1391 - accuracy: 0.5927 - val_loss: 1.2024 - val_accuracy: 0.6017\n",
      "Epoch 3/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 1.0025 - accuracy: 0.6438 - val_loss: 0.8963 - val_accuracy: 0.6919\n",
      "Epoch 4/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.9254 - accuracy: 0.6734 - val_loss: 1.1087 - val_accuracy: 0.6293\n",
      "Epoch 5/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.8765 - accuracy: 0.6916 - val_loss: 0.9029 - val_accuracy: 0.6954\n",
      "Epoch 6/10\n",
      "781/781 [==============================] - 52s 67ms/step - loss: 0.8258 - accuracy: 0.7100 - val_loss: 0.7703 - val_accuracy: 0.7392\n",
      "Epoch 7/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.7918 - accuracy: 0.7225 - val_loss: 0.7755 - val_accuracy: 0.7409\n",
      "Epoch 8/10\n",
      "781/781 [==============================] - 23s 30ms/step - loss: 0.7604 - accuracy: 0.7352 - val_loss: 0.7618 - val_accuracy: 0.7477\n",
      "Epoch 9/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.7346 - accuracy: 0.7435 - val_loss: 0.9406 - val_accuracy: 0.6951\n",
      "Epoch 10/10\n",
      "781/781 [==============================] - 23s 30ms/step - loss: 0.7119 - accuracy: 0.7522 - val_loss: 0.6782 - val_accuracy: 0.7703\n",
      "INFO:tensorflow:Assets written to: models1\\model_1.model\\assets\n",
      "Net 2 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "781/781 [==============================] - 23s 30ms/step - loss: 1.6559 - accuracy: 0.4438 - val_loss: 1.3548 - val_accuracy: 0.5395\n",
      "Epoch 2/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 1.1530 - accuracy: 0.5895 - val_loss: 1.1518 - val_accuracy: 0.6143\n",
      "Epoch 3/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 1.0131 - accuracy: 0.6426 - val_loss: 1.0806 - val_accuracy: 0.6433\n",
      "Epoch 4/10\n",
      "781/781 [==============================] - 23s 30ms/step - loss: 0.9361 - accuracy: 0.6689 - val_loss: 0.8892 - val_accuracy: 0.6956\n",
      "Epoch 5/10\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 0.8819 - accuracy: 0.6906 - val_loss: 1.0689 - val_accuracy: 0.6605\n",
      "Epoch 6/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.8275 - accuracy: 0.7087 - val_loss: 0.7808 - val_accuracy: 0.7314\n",
      "Epoch 7/10\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.7920 - accuracy: 0.7227 - val_loss: 0.9672 - val_accuracy: 0.6828\n",
      "Epoch 8/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.7565 - accuracy: 0.7352 - val_loss: 0.7960 - val_accuracy: 0.7260\n",
      "Epoch 9/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.7312 - accuracy: 0.7434 - val_loss: 0.6682 - val_accuracy: 0.7725\n",
      "Epoch 10/10\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 0.7028 - accuracy: 0.7537 - val_loss: 0.7692 - val_accuracy: 0.7390\n",
      "INFO:tensorflow:Assets written to: models1\\model_2.model\\assets\n",
      "Net 3 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 1.6405 - accuracy: 0.4501 - val_loss: 1.1384 - val_accuracy: 0.5946\n",
      "Epoch 2/10\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 1.1410 - accuracy: 0.5934 - val_loss: 1.1916 - val_accuracy: 0.6036\n",
      "Epoch 3/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 1.0070 - accuracy: 0.6436 - val_loss: 0.8387 - val_accuracy: 0.7070\n",
      "Epoch 4/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.9190 - accuracy: 0.6749 - val_loss: 0.8973 - val_accuracy: 0.7024\n",
      "Epoch 5/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.8706 - accuracy: 0.6921 - val_loss: 0.8266 - val_accuracy: 0.7229\n",
      "Epoch 6/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.8229 - accuracy: 0.7106 - val_loss: 0.8346 - val_accuracy: 0.7135\n",
      "Epoch 7/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.7775 - accuracy: 0.7270 - val_loss: 0.8318 - val_accuracy: 0.7185\n",
      "Epoch 8/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.7532 - accuracy: 0.7357 - val_loss: 0.7186 - val_accuracy: 0.7557\n",
      "Epoch 9/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.7193 - accuracy: 0.7483 - val_loss: 0.7679 - val_accuracy: 0.7396\n",
      "Epoch 10/10\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 0.6959 - accuracy: 0.7565 - val_loss: 0.7396 - val_accuracy: 0.7480\n",
      "INFO:tensorflow:Assets written to: models1\\model_3.model\\assets\n",
      "Net 4 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 1.6184 - accuracy: 0.4567 - val_loss: 1.1779 - val_accuracy: 0.5928\n",
      "Epoch 2/10\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 1.1337 - accuracy: 0.5987 - val_loss: 1.0753 - val_accuracy: 0.6284\n",
      "Epoch 3/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.9915 - accuracy: 0.6490 - val_loss: 0.9227 - val_accuracy: 0.6799\n",
      "Epoch 4/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.9260 - accuracy: 0.6723 - val_loss: 0.9472 - val_accuracy: 0.6884 - l\n",
      "Epoch 5/10\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 0.8618 - accuracy: 0.6978 - val_loss: 0.8872 - val_accuracy: 0.7051\n",
      "Epoch 6/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.8198 - accuracy: 0.7124 - val_loss: 0.8868 - val_accuracy: 0.709920 - ETA\n",
      "Epoch 7/10\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 0.7797 - accuracy: 0.7254 - val_loss: 0.8310 - val_accuracy: 0.7232\n",
      "Epoch 8/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.7476 - accuracy: 0.7375 - val_loss: 0.8495 - val_accuracy: 0.7227\n",
      "Epoch 9/10\n",
      "781/781 [==============================] - 31s 40ms/step - loss: 0.7253 - accuracy: 0.7459 - val_loss: 0.7911 - val_accuracy: 0.7426\n",
      "Epoch 10/10\n",
      "781/781 [==============================] - 31s 40ms/step - loss: 0.7023 - accuracy: 0.7541 - val_loss: 0.7094 - val_accuracy: 0.7671\n",
      "INFO:tensorflow:Assets written to: models1\\model_4.model\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "numberOfModels = 5\n",
    "epochs = 10\n",
    "\n",
    "for i in range(numberOfModels):\n",
    "    print('Net', i, 'is being trained...')\n",
    "    \n",
    "    # choose the optimizer\n",
    "    #opt = SGD(lr = 0.01, decay = 0.1 / epochs, momentum = 0.9, nesterov = True)\n",
    "    opt = Adam()\n",
    "    \n",
    "    # compile the model\n",
    "    model = MiniVGGNet.build(32, 32, 3, 10)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "    \n",
    "    # train the model\n",
    "    H = model.fit(aug.flow(trainX, trainY, batch_size = 64), validation_data = (testX, testY), epochs = epochs, steps_per_epoch = len(trainX) // 64, verbose = 1)\n",
    "    \n",
    "    # save the model\n",
    "    p = ['models1', 'model_{}.model'.format(i)]\n",
    "    model.save(os.path.sep.join(p))\n",
    "    \n",
    "    # evaluate the network\n",
    "    predictions = model.predict(testX, batch_size=64)\n",
    "    report = classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames)\n",
    "    \n",
    "    # save the classification report to file\n",
    "    p = ['output1', 'model_{}.txt'.format(i)]\n",
    "    f = open(os.path.sep.join(p), \"w\")\n",
    "    f.write(report)\n",
    "    f.close()\n",
    "    \n",
    "    # plot the training loss and accuracy\n",
    "    p = ['output1', 'model_{}.png'.format(i)]\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, epochs), H.history['loss'], label = 'train_loss')\n",
    "    plt.plot(np.arange(0, epochs), H.history['val_loss'], label = 'val_loss')\n",
    "    plt.plot(np.arange(0, epochs), H.history['accuracy'], label = 'train_acc')\n",
    "    plt.plot(np.arange(0, epochs), H.history['val_accuracy'], label = 'val_acc')\n",
    "    \n",
    "    # add labels and legend\n",
    "    plt.title('Training Loss and Accuracy for model {}'.format(i))\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Loss/Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # save graphs\n",
    "    plt.savefig(os.path.sep.join(p))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we test the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 1/5\n",
      "Loading model 2/5\n",
      "Loading model 3/5\n",
      "Loading model 4/5\n",
      "Loading model 5/5\n",
      "Evaluating ensemble...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.81      0.86      0.83      1000\n",
      "  automobile       0.90      0.93      0.91      1000\n",
      "        bird       0.79      0.68      0.73      1000\n",
      "         cat       0.74      0.51      0.60      1000\n",
      "        deer       0.74      0.81      0.77      1000\n",
      "         dog       0.86      0.57      0.69      1000\n",
      "        frog       0.64      0.96      0.77      1000\n",
      "       horse       0.85      0.85      0.85      1000\n",
      "        ship       0.92      0.89      0.90      1000\n",
      "       truck       0.83      0.93      0.87      1000\n",
      "\n",
      "    accuracy                           0.80     10000\n",
      "   macro avg       0.81      0.80      0.79     10000\n",
      "weighted avg       0.81      0.80      0.79     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# construct the path used to collect the models then initialize the\n",
    "# models list\n",
    "modelPaths = os.path.sep.join(['models1', '*.model'])\n",
    "modelPaths = list(glob.glob(modelPaths))\n",
    "models = []\n",
    "\n",
    "# loop over the model paths, loading the model, and adding it to\n",
    "# the list of models\n",
    "for (i, modelPath) in enumerate(modelPaths):\n",
    "\tprint('Loading model {}/{}'.format(i + 1, len(modelPaths)))\n",
    "\tmodels.append(load_model(modelPath))\n",
    "\n",
    "# initialize the list of predictions\n",
    "print('Evaluating ensemble...')\n",
    "predictions = []\n",
    "\n",
    "# loop over the models\n",
    "for model in models:\n",
    "\t# use the current model to make predictions on the testing data,\n",
    "\t# then store these predictions in the aggregate predictions list\n",
    "\tpredictions.append(model.predict(testX, batch_size=64))\n",
    "\n",
    "# average the probabilities across all model predictions, then show\n",
    "# a classification report\n",
    "predictions = np.average(predictions, axis=0)\n",
    "print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Here, we actually ran the network with the very same hyperparameters 5 times and found that the best ones performed at around **77%**, but the ensemble of all 5 averaged together reseults in a higher accuracy rate of **80%**!\n",
    "\n",
    "Mathematically, what has happened is likely that the different nets converged to *different* local minima so that, while their individual accuracy rates were all similar, the different nets were, apparently misclassifying *different* test examples, resulting in the average classification of all the models being correct more frequently than any individual net.\n",
    "\n",
    "### Ensemble Training with Different Nets\n",
    "\n",
    "It stands to reason that nets that are more significantly different are more likely to make significantly different classification mistakes since they may work in very different ways. For example, our past GoogLeNet and ResNet experiments had about 90% success on CIFAR-10, but they are very different architectures. Let's try to create an ensemble of those!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "    \n",
    "# load cifar10 data\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "trainX = trainX.astype('float')\n",
    "testX = testX.astype('float')\n",
    "labelNames = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", 'dog', \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "# preprocess data\n",
    "mean = np.mean(trainX, axis = 0)\n",
    "trainX -= mean\n",
    "testX -= mean\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)\n",
    "\n",
    "# create an image generator for data augmentation with random shifting, rotation, and horizontal flips\n",
    "aug = ImageDataGenerator(rotation_range = 10, width_shift_range = 0.1, height_shift_range = 0.1, horizontal_flip = True, fill_mode = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogLeNet 0 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "781/781 [==============================] - 31s 40ms/step - loss: 1.3488 - accuracy: 0.5128 - val_loss: 2.0277 - val_accuracy: 0.4694\n",
      "Epoch 2/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.8970 - accuracy: 0.6853 - val_loss: 0.8755 - val_accuracy: 0.6964\n",
      "Epoch 3/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.7277 - accuracy: 0.7491 - val_loss: 0.7028 - val_accuracy: 0.7612\n",
      "Epoch 4/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.6233 - accuracy: 0.7872 - val_loss: 0.8016 - val_accuracy: 0.7301\n",
      "Epoch 5/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.5506 - accuracy: 0.8116 - val_loss: 0.7213 - val_accuracy: 0.7692\n",
      "Epoch 6/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.5000 - accuracy: 0.8286 - val_loss: 0.8530 - val_accuracy: 0.7382\n",
      "Epoch 7/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4637 - accuracy: 0.8422 - val_loss: 0.7102 - val_accuracy: 0.7776\n",
      "Epoch 8/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4242 - accuracy: 0.8535 - val_loss: 0.6242 - val_accuracy: 0.7977\n",
      "Epoch 9/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3949 - accuracy: 0.8652 - val_loss: 0.6870 - val_accuracy: 0.7858\n",
      "Epoch 10/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3681 - accuracy: 0.8736 - val_loss: 0.7251 - val_accuracy: 0.7773\n",
      "Epoch 11/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3466 - accuracy: 0.8814 - val_loss: 0.5081 - val_accuracy: 0.8294\n",
      "Epoch 12/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3285 - accuracy: 0.8879 - val_loss: 0.3737 - val_accuracy: 0.8785\n",
      "Epoch 13/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3062 - accuracy: 0.8941 - val_loss: 0.6040 - val_accuracy: 0.8086\n",
      "Epoch 14/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2903 - accuracy: 0.8997 - val_loss: 0.4948 - val_accuracy: 0.8446\n",
      "Epoch 15/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2767 - accuracy: 0.9044 - val_loss: 0.4553 - val_accuracy: 0.8526\n",
      "Epoch 16/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2605 - accuracy: 0.9100 - val_loss: 0.6283 - val_accuracy: 0.8136\n",
      "Epoch 17/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2481 - accuracy: 0.9139 - val_loss: 0.6131 - val_accuracy: 0.8231\n",
      "Epoch 18/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.2390 - accuracy: 0.9157 - val_loss: 0.5179 - val_accuracy: 0.8468\n",
      "Epoch 19/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.2252 - accuracy: 0.9225 - val_loss: 0.4201 - val_accuracy: 0.8695\n",
      "Epoch 20/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.2129 - accuracy: 0.9260 - val_loss: 0.3841 - val_accuracy: 0.8845\n",
      "Epoch 21/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.2037 - accuracy: 0.9290 - val_loss: 0.3718 - val_accuracy: 0.8872\n",
      "Epoch 22/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1973 - accuracy: 0.9311 - val_loss: 0.3948 - val_accuracy: 0.8789\n",
      "Epoch 23/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1892 - accuracy: 0.9334 - val_loss: 0.4779 - val_accuracy: 0.8653\n",
      "Epoch 24/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1803 - accuracy: 0.9361 - val_loss: 0.3896 - val_accuracy: 0.8844\n",
      "Epoch 25/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1724 - accuracy: 0.9398 - val_loss: 0.4099 - val_accuracy: 0.8811\n",
      "Epoch 26/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1605 - accuracy: 0.9436 - val_loss: 0.6079 - val_accuracy: 0.8448\n",
      "Epoch 27/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1623 - accuracy: 0.9433 - val_loss: 0.5330 - val_accuracy: 0.8596\n",
      "Epoch 28/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1525 - accuracy: 0.9465 - val_loss: 0.5566 - val_accuracy: 0.8589\n",
      "Epoch 29/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1442 - accuracy: 0.9489 - val_loss: 0.4193 - val_accuracy: 0.8784\n",
      "Epoch 30/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1437 - accuracy: 0.9488 - val_loss: 0.4500 - val_accuracy: 0.8752\n",
      "Epoch 31/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1379 - accuracy: 0.9509 - val_loss: 0.4172 - val_accuracy: 0.8877\n",
      "Epoch 32/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1319 - accuracy: 0.9537 - val_loss: 0.4287 - val_accuracy: 0.8822\n",
      "Epoch 33/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1311 - accuracy: 0.9538 - val_loss: 0.3604 - val_accuracy: 0.8974\n",
      "Epoch 34/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1230 - accuracy: 0.9562 - val_loss: 0.3859 - val_accuracy: 0.8932\n",
      "Epoch 35/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1165 - accuracy: 0.9592 - val_loss: 0.4670 - val_accuracy: 0.8830\n",
      "Epoch 36/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1211 - accuracy: 0.9572 - val_loss: 0.3966 - val_accuracy: 0.8970\n",
      "Epoch 37/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1132 - accuracy: 0.9607 - val_loss: 0.3746 - val_accuracy: 0.9002\n",
      "Epoch 38/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1078 - accuracy: 0.9618 - val_loss: 0.6814 - val_accuracy: 0.8572\n",
      "Epoch 39/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1076 - accuracy: 0.9628 - val_loss: 0.3823 - val_accuracy: 0.9018\n",
      "Epoch 40/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1014 - accuracy: 0.9641 - val_loss: 0.4393 - val_accuracy: 0.8873\n",
      "Epoch 41/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1022 - accuracy: 0.9645 - val_loss: 0.4026 - val_accuracy: 0.8961\n",
      "Epoch 42/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0984 - accuracy: 0.9655 - val_loss: 0.3988 - val_accuracy: 0.8964\n",
      "Epoch 43/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0936 - accuracy: 0.9668 - val_loss: 0.4271 - val_accuracy: 0.8975\n",
      "Epoch 44/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0919 - accuracy: 0.9675 - val_loss: 0.4038 - val_accuracy: 0.8981\n",
      "Epoch 45/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0923 - accuracy: 0.9684 - val_loss: 0.4136 - val_accuracy: 0.8918\n",
      "Epoch 46/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0902 - accuracy: 0.9686 - val_loss: 0.4377 - val_accuracy: 0.8920\n",
      "Epoch 47/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0885 - accuracy: 0.9692 - val_loss: 0.4277 - val_accuracy: 0.9000\n",
      "Epoch 48/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0803 - accuracy: 0.9719 - val_loss: 0.6035 - val_accuracy: 0.8737\n",
      "Epoch 49/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0812 - accuracy: 0.9717 - val_loss: 0.4338 - val_accuracy: 0.9006\n",
      "Epoch 50/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0814 - accuracy: 0.9723 - val_loss: 0.5015 - val_accuracy: 0.8899\n",
      "Epoch 51/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0830 - accuracy: 0.9714 - val_loss: 0.4891 - val_accuracy: 0.8872\n",
      "Epoch 52/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0793 - accuracy: 0.9719 - val_loss: 0.4138 - val_accuracy: 0.8938\n",
      "Epoch 53/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0747 - accuracy: 0.9736 - val_loss: 0.5171 - val_accuracy: 0.8854\n",
      "Epoch 54/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0746 - accuracy: 0.9750 - val_loss: 0.3745 - val_accuracy: 0.9050\n",
      "Epoch 55/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0781 - accuracy: 0.9728 - val_loss: 0.4465 - val_accuracy: 0.8981\n",
      "Epoch 56/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0675 - accuracy: 0.9762 - val_loss: 0.4920 - val_accuracy: 0.8895\n",
      "Epoch 57/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0696 - accuracy: 0.9764 - val_loss: 0.4358 - val_accuracy: 0.8966\n",
      "Epoch 58/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0700 - accuracy: 0.9752 - val_loss: 0.4157 - val_accuracy: 0.9024\n",
      "Epoch 59/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0673 - accuracy: 0.9765 - val_loss: 0.4410 - val_accuracy: 0.8967\n",
      "Epoch 60/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0651 - accuracy: 0.9772 - val_loss: 0.4720 - val_accuracy: 0.8952\n",
      "Epoch 61/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0654 - accuracy: 0.9771 - val_loss: 0.4376 - val_accuracy: 0.9006\n",
      "Epoch 62/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0648 - accuracy: 0.9778 - val_loss: 0.3928 - val_accuracy: 0.9085\n",
      "Epoch 63/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0675 - accuracy: 0.9759 - val_loss: 0.4342 - val_accuracy: 0.9053\n",
      "Epoch 64/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0615 - accuracy: 0.9782 - val_loss: 0.5436 - val_accuracy: 0.8835\n",
      "Epoch 65/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0646 - accuracy: 0.9784 - val_loss: 0.5166 - val_accuracy: 0.8970\n",
      "Epoch 66/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0609 - accuracy: 0.9792 - val_loss: 0.3889 - val_accuracy: 0.9059\n",
      "Epoch 67/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0574 - accuracy: 0.9806 - val_loss: 0.4567 - val_accuracy: 0.8981\n",
      "Epoch 68/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0581 - accuracy: 0.9808 - val_loss: 0.4143 - val_accuracy: 0.9039\n",
      "Epoch 69/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0590 - accuracy: 0.9801 - val_loss: 0.5653 - val_accuracy: 0.8836\n",
      "Epoch 70/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0589 - accuracy: 0.9804 - val_loss: 0.4405 - val_accuracy: 0.9047\n",
      "Epoch 71/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0590 - accuracy: 0.9796 - val_loss: 0.4100 - val_accuracy: 0.9061\n",
      "Epoch 72/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0546 - accuracy: 0.9818 - val_loss: 0.5131 - val_accuracy: 0.8944\n",
      "Epoch 73/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0549 - accuracy: 0.9809 - val_loss: 0.4243 - val_accuracy: 0.9061\n",
      "Epoch 74/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0517 - accuracy: 0.9818 - val_loss: 0.4303 - val_accuracy: 0.9055\n",
      "Epoch 75/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0544 - accuracy: 0.9811 - val_loss: 0.5365 - val_accuracy: 0.8950\n",
      "Epoch 76/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0541 - accuracy: 0.9817 - val_loss: 0.4692 - val_accuracy: 0.9038\n",
      "Epoch 77/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0501 - accuracy: 0.9832 - val_loss: 0.4319 - val_accuracy: 0.9034\n",
      "Epoch 78/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0521 - accuracy: 0.9820 - val_loss: 0.4811 - val_accuracy: 0.9052\n",
      "Epoch 79/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0488 - accuracy: 0.9834 - val_loss: 0.4772 - val_accuracy: 0.9039\n",
      "Epoch 80/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0498 - accuracy: 0.9838 - val_loss: 0.4740 - val_accuracy: 0.8959\n",
      "Epoch 81/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0514 - accuracy: 0.9823 - val_loss: 0.4223 - val_accuracy: 0.9082\n",
      "Epoch 82/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0453 - accuracy: 0.9847 - val_loss: 0.4666 - val_accuracy: 0.9010\n",
      "Epoch 83/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0508 - accuracy: 0.9826 - val_loss: 0.4438 - val_accuracy: 0.9054\n",
      "Epoch 84/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0491 - accuracy: 0.9833 - val_loss: 0.4725 - val_accuracy: 0.9090\n",
      "Epoch 85/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0437 - accuracy: 0.9852 - val_loss: 0.4488 - val_accuracy: 0.9125\n",
      "Epoch 86/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0484 - accuracy: 0.9835 - val_loss: 0.5380 - val_accuracy: 0.8917\n",
      "Epoch 87/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0452 - accuracy: 0.9841 - val_loss: 0.4374 - val_accuracy: 0.9057\n",
      "Epoch 88/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0443 - accuracy: 0.9846 - val_loss: 0.4322 - val_accuracy: 0.9062\n",
      "Epoch 89/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0492 - accuracy: 0.9835 - val_loss: 0.4872 - val_accuracy: 0.9016\n",
      "Epoch 90/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0424 - accuracy: 0.9853 - val_loss: 0.4733 - val_accuracy: 0.9060\n",
      "Epoch 91/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0461 - accuracy: 0.9839 - val_loss: 0.4844 - val_accuracy: 0.9014\n",
      "Epoch 92/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0430 - accuracy: 0.9849 - val_loss: 0.4510 - val_accuracy: 0.9072\n",
      "Epoch 93/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0426 - accuracy: 0.9855 - val_loss: 0.4207 - val_accuracy: 0.9089\n",
      "Epoch 94/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0419 - accuracy: 0.9854 - val_loss: 0.5389 - val_accuracy: 0.8987\n",
      "Epoch 95/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0420 - accuracy: 0.9859 - val_loss: 0.4578 - val_accuracy: 0.9028\n",
      "Epoch 96/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0367 - accuracy: 0.9874 - val_loss: 0.5120 - val_accuracy: 0.9025\n",
      "Epoch 97/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0428 - accuracy: 0.9856 - val_loss: 0.4898 - val_accuracy: 0.9060\n",
      "Epoch 98/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0433 - accuracy: 0.9852 - val_loss: 0.4704 - val_accuracy: 0.9066\n",
      "Epoch 99/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0406 - accuracy: 0.9861 - val_loss: 0.4434 - val_accuracy: 0.9079\n",
      "Epoch 100/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0390 - accuracy: 0.9867 - val_loss: 0.4889 - val_accuracy: 0.9017\n",
      "INFO:tensorflow:Assets written to: models2\\model_0.model\\assets\n",
      "GoogLeNet 1 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "781/781 [==============================] - 30s 39ms/step - loss: 1.4514 - accuracy: 0.4696 - val_loss: 1.1475 - val_accuracy: 0.5822\n",
      "Epoch 2/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.9995 - accuracy: 0.6465 - val_loss: 0.8182 - val_accuracy: 0.7164\n",
      "Epoch 3/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.8152 - accuracy: 0.7158 - val_loss: 0.7897 - val_accuracy: 0.7260\n",
      "Epoch 4/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.6848 - accuracy: 0.7633 - val_loss: 1.0064 - val_accuracy: 0.6853\n",
      "Epoch 5/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.6049 - accuracy: 0.7944 - val_loss: 0.8445 - val_accuracy: 0.7147\n",
      "Epoch 6/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5470 - accuracy: 0.8129 - val_loss: 0.7423 - val_accuracy: 0.7560\n",
      "Epoch 7/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4947 - accuracy: 0.8300 - val_loss: 0.5878 - val_accuracy: 0.8004\n",
      "Epoch 8/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4626 - accuracy: 0.8426 - val_loss: 0.4870 - val_accuracy: 0.8400\n",
      "Epoch 9/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4276 - accuracy: 0.8528 - val_loss: 0.5702 - val_accuracy: 0.8038\n",
      "Epoch 10/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4044 - accuracy: 0.8608 - val_loss: 0.4216 - val_accuracy: 0.8578\n",
      "Epoch 11/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3732 - accuracy: 0.8712 - val_loss: 0.5204 - val_accuracy: 0.8282\n",
      "Epoch 12/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3515 - accuracy: 0.8795 - val_loss: 0.7327 - val_accuracy: 0.7776\n",
      "Epoch 13/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3372 - accuracy: 0.8847 - val_loss: 0.4795 - val_accuracy: 0.8410\n",
      "Epoch 14/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3158 - accuracy: 0.8906 - val_loss: 0.4133 - val_accuracy: 0.8595\n",
      "Epoch 15/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3029 - accuracy: 0.8952 - val_loss: 0.4774 - val_accuracy: 0.8533\n",
      "Epoch 16/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.2821 - accuracy: 0.9027 - val_loss: 0.4983 - val_accuracy: 0.8467\n",
      "Epoch 17/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2663 - accuracy: 0.9072 - val_loss: 0.4748 - val_accuracy: 0.8485\n",
      "Epoch 18/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2581 - accuracy: 0.9111 - val_loss: 0.3640 - val_accuracy: 0.8853\n",
      "Epoch 19/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2446 - accuracy: 0.9159 - val_loss: 0.4759 - val_accuracy: 0.8546\n",
      "Epoch 20/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2332 - accuracy: 0.9191 - val_loss: 0.5409 - val_accuracy: 0.8473\n",
      "Epoch 21/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.2279 - accuracy: 0.9208 - val_loss: 0.4160 - val_accuracy: 0.8697\n",
      "Epoch 22/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2119 - accuracy: 0.9263 - val_loss: 0.4622 - val_accuracy: 0.8653\n",
      "Epoch 23/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2026 - accuracy: 0.9294 - val_loss: 0.4333 - val_accuracy: 0.8758\n",
      "Epoch 24/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.1943 - accuracy: 0.9317 - val_loss: 0.4421 - val_accuracy: 0.8703\n",
      "Epoch 25/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.1862 - accuracy: 0.9353 - val_loss: 0.5245 - val_accuracy: 0.8541\n",
      "Epoch 26/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.1825 - accuracy: 0.9362 - val_loss: 0.3994 - val_accuracy: 0.8784\n",
      "Epoch 27/100\n",
      "781/781 [==============================] - 29s 38ms/step - loss: 0.1714 - accuracy: 0.9403 - val_loss: 0.4550 - val_accuracy: 0.8789\n",
      "Epoch 28/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.1680 - accuracy: 0.9415 - val_loss: 0.4356 - val_accuracy: 0.8686\n",
      "Epoch 29/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1586 - accuracy: 0.9438 - val_loss: 0.4311 - val_accuracy: 0.8722\n",
      "Epoch 30/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1558 - accuracy: 0.9452 - val_loss: 0.6042 - val_accuracy: 0.8439\n",
      "Epoch 31/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1476 - accuracy: 0.9480 - val_loss: 0.5505 - val_accuracy: 0.8573\n",
      "Epoch 32/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1447 - accuracy: 0.9482 - val_loss: 0.4867 - val_accuracy: 0.8783\n",
      "Epoch 33/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1397 - accuracy: 0.9502 - val_loss: 0.4765 - val_accuracy: 0.8745\n",
      "Epoch 34/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1334 - accuracy: 0.9534 - val_loss: 0.3944 - val_accuracy: 0.8906\n",
      "Epoch 35/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1308 - accuracy: 0.9542 - val_loss: 0.3801 - val_accuracy: 0.8889\n",
      "Epoch 36/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1269 - accuracy: 0.9554 - val_loss: 0.4125 - val_accuracy: 0.8895\n",
      "Epoch 37/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1224 - accuracy: 0.9567 - val_loss: 0.4900 - val_accuracy: 0.8715\n",
      "Epoch 38/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1153 - accuracy: 0.9585 - val_loss: 0.3544 - val_accuracy: 0.8955\n",
      "Epoch 39/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1178 - accuracy: 0.9583 - val_loss: 0.4350 - val_accuracy: 0.8805\n",
      "Epoch 40/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1125 - accuracy: 0.9604 - val_loss: 0.4197 - val_accuracy: 0.8911\n",
      "Epoch 41/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1097 - accuracy: 0.9615 - val_loss: 0.4528 - val_accuracy: 0.8832\n",
      "Epoch 42/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1062 - accuracy: 0.9621 - val_loss: 0.4377 - val_accuracy: 0.8825\n",
      "Epoch 43/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1030 - accuracy: 0.9640 - val_loss: 0.4928 - val_accuracy: 0.8810\n",
      "Epoch 44/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1024 - accuracy: 0.9635 - val_loss: 0.4442 - val_accuracy: 0.8837\n",
      "Epoch 45/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0966 - accuracy: 0.9654 - val_loss: 0.4523 - val_accuracy: 0.8853\n",
      "Epoch 46/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0922 - accuracy: 0.9680 - val_loss: 0.4071 - val_accuracy: 0.8969\n",
      "Epoch 47/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0950 - accuracy: 0.9662 - val_loss: 0.5126 - val_accuracy: 0.8787\n",
      "Epoch 48/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0865 - accuracy: 0.9694 - val_loss: 0.4044 - val_accuracy: 0.8988\n",
      "Epoch 49/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0910 - accuracy: 0.9677 - val_loss: 0.4438 - val_accuracy: 0.8875\n",
      "Epoch 50/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0890 - accuracy: 0.9685 - val_loss: 0.4485 - val_accuracy: 0.8894\n",
      "Epoch 51/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0849 - accuracy: 0.9704 - val_loss: 0.5904 - val_accuracy: 0.8694\n",
      "Epoch 52/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0855 - accuracy: 0.9698 - val_loss: 0.4282 - val_accuracy: 0.8943\n",
      "Epoch 53/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0850 - accuracy: 0.9708 - val_loss: 0.4532 - val_accuracy: 0.8824\n",
      "Epoch 54/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0796 - accuracy: 0.9728 - val_loss: 0.4631 - val_accuracy: 0.8867\n",
      "Epoch 55/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0788 - accuracy: 0.9723 - val_loss: 0.4536 - val_accuracy: 0.8931\n",
      "Epoch 56/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0793 - accuracy: 0.9721 - val_loss: 0.4542 - val_accuracy: 0.8945\n",
      "Epoch 57/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0771 - accuracy: 0.9718 - val_loss: 0.4347 - val_accuracy: 0.9004\n",
      "Epoch 58/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0735 - accuracy: 0.9746 - val_loss: 0.4219 - val_accuracy: 0.8927\n",
      "Epoch 59/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0736 - accuracy: 0.9752 - val_loss: 0.5068 - val_accuracy: 0.8897\n",
      "Epoch 60/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0705 - accuracy: 0.9757 - val_loss: 0.4158 - val_accuracy: 0.9033\n",
      "Epoch 61/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0732 - accuracy: 0.9749 - val_loss: 0.4575 - val_accuracy: 0.8934\n",
      "Epoch 62/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0684 - accuracy: 0.9768 - val_loss: 0.5284 - val_accuracy: 0.8849\n",
      "Epoch 63/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0726 - accuracy: 0.9742 - val_loss: 0.4812 - val_accuracy: 0.8999\n",
      "Epoch 64/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0635 - accuracy: 0.9781 - val_loss: 0.5953 - val_accuracy: 0.8830\n",
      "Epoch 65/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0665 - accuracy: 0.9769 - val_loss: 0.5687 - val_accuracy: 0.8851\n",
      "Epoch 66/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0699 - accuracy: 0.9762 - val_loss: 0.4570 - val_accuracy: 0.8974\n",
      "Epoch 67/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0636 - accuracy: 0.9776 - val_loss: 0.5207 - val_accuracy: 0.8901\n",
      "Epoch 68/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0615 - accuracy: 0.9788 - val_loss: 0.5135 - val_accuracy: 0.8960\n",
      "Epoch 69/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0615 - accuracy: 0.9781 - val_loss: 0.4663 - val_accuracy: 0.8971\n",
      "Epoch 70/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0603 - accuracy: 0.9795 - val_loss: 0.4484 - val_accuracy: 0.9007\n",
      "Epoch 71/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0604 - accuracy: 0.9783 - val_loss: 0.4567 - val_accuracy: 0.9005\n",
      "Epoch 72/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0579 - accuracy: 0.9801 - val_loss: 0.6181 - val_accuracy: 0.8819\n",
      "Epoch 73/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0574 - accuracy: 0.9806 - val_loss: 0.4929 - val_accuracy: 0.9036\n",
      "Epoch 74/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0548 - accuracy: 0.9809 - val_loss: 0.4473 - val_accuracy: 0.9059\n",
      "Epoch 75/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0565 - accuracy: 0.9800 - val_loss: 0.4874 - val_accuracy: 0.9017\n",
      "Epoch 76/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0551 - accuracy: 0.9810 - val_loss: 0.4378 - val_accuracy: 0.9028\n",
      "Epoch 77/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0560 - accuracy: 0.9804 - val_loss: 0.4813 - val_accuracy: 0.8956\n",
      "Epoch 78/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0553 - accuracy: 0.9805 - val_loss: 0.5308 - val_accuracy: 0.8963\n",
      "Epoch 79/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0542 - accuracy: 0.9813 - val_loss: 0.4860 - val_accuracy: 0.8967\n",
      "Epoch 80/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0495 - accuracy: 0.9831 - val_loss: 0.5194 - val_accuracy: 0.8922\n",
      "Epoch 81/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0498 - accuracy: 0.9822 - val_loss: 0.4334 - val_accuracy: 0.9076\n",
      "Epoch 82/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0534 - accuracy: 0.9815 - val_loss: 0.4690 - val_accuracy: 0.9027\n",
      "Epoch 83/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0541 - accuracy: 0.9813 - val_loss: 0.4667 - val_accuracy: 0.8981\n",
      "Epoch 84/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0486 - accuracy: 0.9829 - val_loss: 0.4436 - val_accuracy: 0.9011\n",
      "Epoch 85/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0498 - accuracy: 0.9829 - val_loss: 0.4872 - val_accuracy: 0.8948\n",
      "Epoch 86/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0503 - accuracy: 0.9832 - val_loss: 0.5638 - val_accuracy: 0.8925\n",
      "Epoch 87/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0492 - accuracy: 0.9832 - val_loss: 0.4495 - val_accuracy: 0.9027\n",
      "Epoch 88/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0440 - accuracy: 0.9852 - val_loss: 0.5623 - val_accuracy: 0.8910\n",
      "Epoch 89/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0505 - accuracy: 0.9827 - val_loss: 0.5257 - val_accuracy: 0.8925\n",
      "Epoch 90/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0465 - accuracy: 0.9840 - val_loss: 0.5290 - val_accuracy: 0.8972\n",
      "Epoch 91/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0476 - accuracy: 0.9833 - val_loss: 0.5412 - val_accuracy: 0.8868\n",
      "Epoch 92/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0454 - accuracy: 0.9846 - val_loss: 0.5121 - val_accuracy: 0.9006\n",
      "Epoch 93/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0471 - accuracy: 0.9839 - val_loss: 0.5071 - val_accuracy: 0.9028\n",
      "Epoch 94/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0440 - accuracy: 0.9850 - val_loss: 0.4853 - val_accuracy: 0.8978\n",
      "Epoch 95/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0451 - accuracy: 0.9845 - val_loss: 0.5159 - val_accuracy: 0.8963\n",
      "Epoch 96/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0437 - accuracy: 0.9845 - val_loss: 0.4623 - val_accuracy: 0.9061\n",
      "Epoch 97/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0422 - accuracy: 0.9852 - val_loss: 0.5638 - val_accuracy: 0.8927\n",
      "Epoch 98/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0454 - accuracy: 0.9848 - val_loss: 0.5611 - val_accuracy: 0.8914\n",
      "Epoch 99/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0453 - accuracy: 0.9846 - val_loss: 0.5295 - val_accuracy: 0.8981\n",
      "Epoch 100/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0407 - accuracy: 0.9862 - val_loss: 0.4609 - val_accuracy: 0.9045\n",
      "INFO:tensorflow:Assets written to: models2\\model_1.model\\assets\n",
      "ResNet 2 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "781/781 [==============================] - 76s 97ms/step - loss: 1.8631 - accuracy: 0.4380 - val_loss: 1.8144 - val_accuracy: 0.4840\n",
      "Epoch 2/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 1.3074 - accuracy: 0.6275 - val_loss: 1.2244 - val_accuracy: 0.6608\n",
      "Epoch 3/100\n",
      "781/781 [==============================] - 65s 84ms/step - loss: 1.1097 - accuracy: 0.6984 - val_loss: 1.1312 - val_accuracy: 0.7021\n",
      "Epoch 4/100\n",
      "781/781 [==============================] - 65s 84ms/step - loss: 0.9826 - accuracy: 0.7422 - val_loss: 1.0237 - val_accuracy: 0.7336\n",
      "Epoch 5/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.9123 - accuracy: 0.7664 - val_loss: 0.9432 - val_accuracy: 0.7675\n",
      "Epoch 6/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.8580 - accuracy: 0.7851 - val_loss: 0.9200 - val_accuracy: 0.7747\n",
      "Epoch 7/100\n",
      "781/781 [==============================] - 65s 84ms/step - loss: 0.8177 - accuracy: 0.8013 - val_loss: 0.8841 - val_accuracy: 0.7852\n",
      "Epoch 8/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7850 - accuracy: 0.8103 - val_loss: 0.8932 - val_accuracy: 0.7825\n",
      "Epoch 9/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7560 - accuracy: 0.8207 - val_loss: 0.9518 - val_accuracy: 0.7614\n",
      "Epoch 10/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7375 - accuracy: 0.8246 - val_loss: 0.8646 - val_accuracy: 0.7927\n",
      "Epoch 11/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7133 - accuracy: 0.8366 - val_loss: 0.8219 - val_accuracy: 0.8069\n",
      "Epoch 12/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7003 - accuracy: 0.8381 - val_loss: 0.8647 - val_accuracy: 0.7982\n",
      "Epoch 13/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6860 - accuracy: 0.8434 - val_loss: 0.9090 - val_accuracy: 0.7855\n",
      "Epoch 14/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6696 - accuracy: 0.8495 - val_loss: 0.7049 - val_accuracy: 0.8410\n",
      "Epoch 15/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6549 - accuracy: 0.8546 - val_loss: 0.6822 - val_accuracy: 0.8397\n",
      "Epoch 16/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6459 - accuracy: 0.8573 - val_loss: 0.8086 - val_accuracy: 0.8163\n",
      "Epoch 17/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6396 - accuracy: 0.8587 - val_loss: 0.7837 - val_accuracy: 0.8170\n",
      "Epoch 18/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6274 - accuracy: 0.8630 - val_loss: 0.7779 - val_accuracy: 0.8201\n",
      "Epoch 19/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6195 - accuracy: 0.8670 - val_loss: 0.7379 - val_accuracy: 0.8285\n",
      "Epoch 20/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6083 - accuracy: 0.8698 - val_loss: 0.6733 - val_accuracy: 0.8520\n",
      "Epoch 21/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6021 - accuracy: 0.8713 - val_loss: 0.6376 - val_accuracy: 0.8627\n",
      "Epoch 22/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5916 - accuracy: 0.8746 - val_loss: 0.6962 - val_accuracy: 0.8463\n",
      "Epoch 23/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5908 - accuracy: 0.8756 - val_loss: 0.7456 - val_accuracy: 0.8277\n",
      "Epoch 24/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5798 - accuracy: 0.8775 - val_loss: 0.6775 - val_accuracy: 0.8508\n",
      "Epoch 25/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5728 - accuracy: 0.8809 - val_loss: 0.6564 - val_accuracy: 0.8585\n",
      "Epoch 26/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5717 - accuracy: 0.8814 - val_loss: 0.7176 - val_accuracy: 0.8419\n",
      "Epoch 27/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5686 - accuracy: 0.8821 - val_loss: 0.6701 - val_accuracy: 0.8537\n",
      "Epoch 28/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5573 - accuracy: 0.8866 - val_loss: 0.6797 - val_accuracy: 0.8470\n",
      "Epoch 29/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5522 - accuracy: 0.8889 - val_loss: 0.6748 - val_accuracy: 0.8584\n",
      "Epoch 30/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5514 - accuracy: 0.8882 - val_loss: 0.6173 - val_accuracy: 0.8706\n",
      "Epoch 31/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5457 - accuracy: 0.8905 - val_loss: 0.6898 - val_accuracy: 0.8491\n",
      "Epoch 32/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5446 - accuracy: 0.8905 - val_loss: 0.7335 - val_accuracy: 0.8371\n",
      "Epoch 33/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5442 - accuracy: 0.8919 - val_loss: 0.6491 - val_accuracy: 0.8631\n",
      "Epoch 34/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5350 - accuracy: 0.8940 - val_loss: 0.6163 - val_accuracy: 0.8705\n",
      "Epoch 35/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5271 - accuracy: 0.8967 - val_loss: 0.6937 - val_accuracy: 0.8513\n",
      "Epoch 36/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5284 - accuracy: 0.8959 - val_loss: 0.6300 - val_accuracy: 0.8677\n",
      "Epoch 37/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5248 - accuracy: 0.8954 - val_loss: 0.6087 - val_accuracy: 0.8715\n",
      "Epoch 38/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5208 - accuracy: 0.8970 - val_loss: 0.6421 - val_accuracy: 0.8640\n",
      "Epoch 39/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5192 - accuracy: 0.8982 - val_loss: 0.6311 - val_accuracy: 0.8695\n",
      "Epoch 40/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5175 - accuracy: 0.8994 - val_loss: 0.6293 - val_accuracy: 0.8666\n",
      "Epoch 41/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5108 - accuracy: 0.9010 - val_loss: 0.6045 - val_accuracy: 0.8778\n",
      "Epoch 42/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5128 - accuracy: 0.9005 - val_loss: 0.6277 - val_accuracy: 0.8683\n",
      "Epoch 43/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5104 - accuracy: 0.9002 - val_loss: 0.6566 - val_accuracy: 0.8607\n",
      "Epoch 44/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5067 - accuracy: 0.9018 - val_loss: 0.6869 - val_accuracy: 0.8474\n",
      "Epoch 45/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5073 - accuracy: 0.9010 - val_loss: 0.6283 - val_accuracy: 0.8741\n",
      "Epoch 46/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5024 - accuracy: 0.9054 - val_loss: 0.6285 - val_accuracy: 0.8682\n",
      "Epoch 47/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4993 - accuracy: 0.9050 - val_loss: 0.5581 - val_accuracy: 0.8860\n",
      "Epoch 48/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4970 - accuracy: 0.9058 - val_loss: 0.6204 - val_accuracy: 0.8723\n",
      "Epoch 49/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4965 - accuracy: 0.9059 - val_loss: 0.6862 - val_accuracy: 0.8548\n",
      "Epoch 50/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4883 - accuracy: 0.9087 - val_loss: 0.6207 - val_accuracy: 0.8746\n",
      "Epoch 51/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4945 - accuracy: 0.9064 - val_loss: 0.5941 - val_accuracy: 0.8763\n",
      "Epoch 52/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4882 - accuracy: 0.9087 - val_loss: 0.5865 - val_accuracy: 0.8817\n",
      "Epoch 53/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4876 - accuracy: 0.9065 - val_loss: 0.5905 - val_accuracy: 0.8841\n",
      "Epoch 54/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4843 - accuracy: 0.9084 - val_loss: 0.6143 - val_accuracy: 0.8703\n",
      "Epoch 55/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4812 - accuracy: 0.9108 - val_loss: 0.6205 - val_accuracy: 0.8693\n",
      "Epoch 56/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4841 - accuracy: 0.9075 - val_loss: 0.5546 - val_accuracy: 0.8913\n",
      "Epoch 57/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4741 - accuracy: 0.9114 - val_loss: 0.6053 - val_accuracy: 0.8722\n",
      "Epoch 58/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4790 - accuracy: 0.9112 - val_loss: 0.6122 - val_accuracy: 0.8726\n",
      "Epoch 59/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4748 - accuracy: 0.9116 - val_loss: 0.6241 - val_accuracy: 0.8709\n",
      "Epoch 60/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4721 - accuracy: 0.9123 - val_loss: 0.5854 - val_accuracy: 0.8822\n",
      "Epoch 61/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4710 - accuracy: 0.9136 - val_loss: 0.5858 - val_accuracy: 0.8847\n",
      "Epoch 62/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4685 - accuracy: 0.9138 - val_loss: 0.5629 - val_accuracy: 0.8856\n",
      "Epoch 63/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4699 - accuracy: 0.9134 - val_loss: 0.6211 - val_accuracy: 0.8775\n",
      "Epoch 64/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4668 - accuracy: 0.9137 - val_loss: 0.5882 - val_accuracy: 0.8817\n",
      "Epoch 65/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4623 - accuracy: 0.9156 - val_loss: 0.6142 - val_accuracy: 0.8775\n",
      "Epoch 66/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4640 - accuracy: 0.9163 - val_loss: 0.5912 - val_accuracy: 0.8822\n",
      "Epoch 67/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4571 - accuracy: 0.9165 - val_loss: 0.5919 - val_accuracy: 0.8785\n",
      "Epoch 68/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4591 - accuracy: 0.9154 - val_loss: 0.6635 - val_accuracy: 0.8628\n",
      "Epoch 69/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4604 - accuracy: 0.9165 - val_loss: 0.5725 - val_accuracy: 0.8872\n",
      "Epoch 70/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4580 - accuracy: 0.9169 - val_loss: 0.5807 - val_accuracy: 0.8857\n",
      "Epoch 71/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4539 - accuracy: 0.9181 - val_loss: 0.6158 - val_accuracy: 0.8765\n",
      "Epoch 72/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4526 - accuracy: 0.9189 - val_loss: 0.6030 - val_accuracy: 0.8790\n",
      "Epoch 73/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4505 - accuracy: 0.9190 - val_loss: 0.6085 - val_accuracy: 0.8781\n",
      "Epoch 74/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4507 - accuracy: 0.9186 - val_loss: 0.5774 - val_accuracy: 0.8819\n",
      "Epoch 75/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4495 - accuracy: 0.9193 - val_loss: 0.6349 - val_accuracy: 0.8684\n",
      "Epoch 76/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4475 - accuracy: 0.9190 - val_loss: 0.6264 - val_accuracy: 0.8753\n",
      "Epoch 77/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4462 - accuracy: 0.9212 - val_loss: 0.5981 - val_accuracy: 0.8822\n",
      "Epoch 78/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4470 - accuracy: 0.9194 - val_loss: 0.6188 - val_accuracy: 0.8754\n",
      "Epoch 79/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4462 - accuracy: 0.9196 - val_loss: 0.6280 - val_accuracy: 0.8745\n",
      "Epoch 80/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4464 - accuracy: 0.9192 - val_loss: 0.6167 - val_accuracy: 0.8770\n",
      "Epoch 81/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4431 - accuracy: 0.9202 - val_loss: 0.5611 - val_accuracy: 0.8893\n",
      "Epoch 82/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4432 - accuracy: 0.9214 - val_loss: 0.5901 - val_accuracy: 0.8818\n",
      "Epoch 83/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4393 - accuracy: 0.9218 - val_loss: 0.6012 - val_accuracy: 0.8805\n",
      "Epoch 84/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4372 - accuracy: 0.9218 - val_loss: 0.5373 - val_accuracy: 0.8949\n",
      "Epoch 85/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4346 - accuracy: 0.9223 - val_loss: 0.6064 - val_accuracy: 0.8784\n",
      "Epoch 86/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4354 - accuracy: 0.9221 - val_loss: 0.6834 - val_accuracy: 0.8602\n",
      "Epoch 87/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4378 - accuracy: 0.9212 - val_loss: 0.5626 - val_accuracy: 0.8885\n",
      "Epoch 88/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4332 - accuracy: 0.9218 - val_loss: 0.5580 - val_accuracy: 0.8915\n",
      "Epoch 89/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4351 - accuracy: 0.9216 - val_loss: 0.6093 - val_accuracy: 0.8797\n",
      "Epoch 90/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4331 - accuracy: 0.9221 - val_loss: 0.5625 - val_accuracy: 0.8883\n",
      "Epoch 91/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4264 - accuracy: 0.9253 - val_loss: 0.5389 - val_accuracy: 0.8945\n",
      "Epoch 92/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4330 - accuracy: 0.9231 - val_loss: 0.5508 - val_accuracy: 0.8925\n",
      "Epoch 93/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4267 - accuracy: 0.9244 - val_loss: 0.5509 - val_accuracy: 0.8886\n",
      "Epoch 94/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4271 - accuracy: 0.9243 - val_loss: 0.5602 - val_accuracy: 0.8891\n",
      "Epoch 95/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4261 - accuracy: 0.9244 - val_loss: 0.5675 - val_accuracy: 0.8868\n",
      "Epoch 96/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4297 - accuracy: 0.9237 - val_loss: 0.6033 - val_accuracy: 0.8772\n",
      "Epoch 97/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4258 - accuracy: 0.9251 - val_loss: 0.6244 - val_accuracy: 0.8735\n",
      "Epoch 98/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4250 - accuracy: 0.9249 - val_loss: 0.5957 - val_accuracy: 0.8791\n",
      "Epoch 99/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4235 - accuracy: 0.9243 - val_loss: 0.5815 - val_accuracy: 0.8843\n",
      "Epoch 100/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4239 - accuracy: 0.9245 - val_loss: 0.5446 - val_accuracy: 0.8946\n",
      "INFO:tensorflow:Assets written to: models2\\model_2.model\\assets\n",
      "ResNet 3 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "781/781 [==============================] - 74s 94ms/step - loss: 1.8830 - accuracy: 0.4308 - val_loss: 1.7444 - val_accuracy: 0.4731\n",
      "Epoch 2/100\n",
      "781/781 [==============================] - 64s 83ms/step - loss: 1.3313 - accuracy: 0.6195 - val_loss: 1.2948 - val_accuracy: 0.6347\n",
      "Epoch 3/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 1.1226 - accuracy: 0.6918 - val_loss: 1.1531 - val_accuracy: 0.6876\n",
      "Epoch 4/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 1.0059 - accuracy: 0.7350 - val_loss: 1.1540 - val_accuracy: 0.6881\n",
      "Epoch 5/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.9137 - accuracy: 0.7697 - val_loss: 1.0139 - val_accuracy: 0.7520\n",
      "Epoch 6/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.8653 - accuracy: 0.7861 - val_loss: 0.9415 - val_accuracy: 0.7619\n",
      "Epoch 7/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.8257 - accuracy: 0.7991 - val_loss: 1.0203 - val_accuracy: 0.7462\n",
      "Epoch 8/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7901 - accuracy: 0.8118 - val_loss: 0.8541 - val_accuracy: 0.7918\n",
      "Epoch 9/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7626 - accuracy: 0.8218 - val_loss: 0.8655 - val_accuracy: 0.7877\n",
      "Epoch 10/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7467 - accuracy: 0.8243 - val_loss: 0.7796 - val_accuracy: 0.8096\n",
      "Epoch 11/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7239 - accuracy: 0.8334 - val_loss: 0.8481 - val_accuracy: 0.7958\n",
      "Epoch 12/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7044 - accuracy: 0.8395 - val_loss: 0.8144 - val_accuracy: 0.8086\n",
      "Epoch 13/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6922 - accuracy: 0.8429 - val_loss: 0.7682 - val_accuracy: 0.8213\n",
      "Epoch 14/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6747 - accuracy: 0.8483 - val_loss: 0.8615 - val_accuracy: 0.7948\n",
      "Epoch 15/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6619 - accuracy: 0.8539 - val_loss: 0.7023 - val_accuracy: 0.8388\n",
      "Epoch 16/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6528 - accuracy: 0.8553 - val_loss: 0.7099 - val_accuracy: 0.8355\n",
      "Epoch 17/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6453 - accuracy: 0.8582 - val_loss: 0.7411 - val_accuracy: 0.8271\n",
      "Epoch 18/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6298 - accuracy: 0.8629 - val_loss: 0.7629 - val_accuracy: 0.8240\n",
      "Epoch 19/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6206 - accuracy: 0.8659 - val_loss: 0.7072 - val_accuracy: 0.8385\n",
      "Epoch 20/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6125 - accuracy: 0.8678 - val_loss: 0.7787 - val_accuracy: 0.8176\n",
      "Epoch 21/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6067 - accuracy: 0.8706 - val_loss: 0.8452 - val_accuracy: 0.8081\n",
      "Epoch 22/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5978 - accuracy: 0.8730 - val_loss: 0.7179 - val_accuracy: 0.8400\n",
      "Epoch 23/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5916 - accuracy: 0.8764 - val_loss: 0.6601 - val_accuracy: 0.8516\n",
      "Epoch 24/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5811 - accuracy: 0.8777 - val_loss: 0.7291 - val_accuracy: 0.8392\n",
      "Epoch 25/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5778 - accuracy: 0.8791 - val_loss: 0.6543 - val_accuracy: 0.8627\n",
      "Epoch 26/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5723 - accuracy: 0.8813 - val_loss: 0.7420 - val_accuracy: 0.8335\n",
      "Epoch 27/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5679 - accuracy: 0.8820 - val_loss: 0.6640 - val_accuracy: 0.8526\n",
      "Epoch 28/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5591 - accuracy: 0.8858 - val_loss: 0.7656 - val_accuracy: 0.8283\n",
      "Epoch 29/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5550 - accuracy: 0.8861 - val_loss: 0.6517 - val_accuracy: 0.8582\n",
      "Epoch 30/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5508 - accuracy: 0.8885 - val_loss: 0.7887 - val_accuracy: 0.8228\n",
      "Epoch 31/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5461 - accuracy: 0.8901 - val_loss: 0.6778 - val_accuracy: 0.8517\n",
      "Epoch 32/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5413 - accuracy: 0.8910 - val_loss: 0.6209 - val_accuracy: 0.8670\n",
      "Epoch 33/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5388 - accuracy: 0.8918 - val_loss: 0.6643 - val_accuracy: 0.8535\n",
      "Epoch 34/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5346 - accuracy: 0.8922 - val_loss: 0.7164 - val_accuracy: 0.8404\n",
      "Epoch 35/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5316 - accuracy: 0.8940 - val_loss: 0.6788 - val_accuracy: 0.8515\n",
      "Epoch 36/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5307 - accuracy: 0.8930 - val_loss: 0.6903 - val_accuracy: 0.8499\n",
      "Epoch 37/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5252 - accuracy: 0.8958 - val_loss: 0.5977 - val_accuracy: 0.8762\n",
      "Epoch 38/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5154 - accuracy: 0.9002 - val_loss: 0.6262 - val_accuracy: 0.8700\n",
      "Epoch 39/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5202 - accuracy: 0.8948 - val_loss: 0.6134 - val_accuracy: 0.8713\n",
      "Epoch 40/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5163 - accuracy: 0.8987 - val_loss: 0.6511 - val_accuracy: 0.8593\n",
      "Epoch 41/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5124 - accuracy: 0.8987 - val_loss: 0.6667 - val_accuracy: 0.8605\n",
      "Epoch 42/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5063 - accuracy: 0.9017 - val_loss: 0.6162 - val_accuracy: 0.8706\n",
      "Epoch 43/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5060 - accuracy: 0.8998 - val_loss: 0.7083 - val_accuracy: 0.8517\n",
      "Epoch 44/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5074 - accuracy: 0.9008 - val_loss: 0.6909 - val_accuracy: 0.8521\n",
      "Epoch 45/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5003 - accuracy: 0.9028 - val_loss: 0.6572 - val_accuracy: 0.8616\n",
      "Epoch 46/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4987 - accuracy: 0.9030 - val_loss: 0.6520 - val_accuracy: 0.8649\n",
      "Epoch 47/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4962 - accuracy: 0.9045 - val_loss: 0.6632 - val_accuracy: 0.8633\n",
      "Epoch 48/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4943 - accuracy: 0.9050 - val_loss: 0.6229 - val_accuracy: 0.8704\n",
      "Epoch 49/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4912 - accuracy: 0.9066 - val_loss: 0.6265 - val_accuracy: 0.8725\n",
      "Epoch 50/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4885 - accuracy: 0.9058 - val_loss: 0.6336 - val_accuracy: 0.8685\n",
      "Epoch 51/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4843 - accuracy: 0.9091 - val_loss: 0.6498 - val_accuracy: 0.8590\n",
      "Epoch 52/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4845 - accuracy: 0.9081 - val_loss: 0.6561 - val_accuracy: 0.8655\n",
      "Epoch 53/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4822 - accuracy: 0.9090 - val_loss: 0.5914 - val_accuracy: 0.8780\n",
      "Epoch 54/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4810 - accuracy: 0.9082 - val_loss: 0.6554 - val_accuracy: 0.8607\n",
      "Epoch 55/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4776 - accuracy: 0.9107 - val_loss: 0.6046 - val_accuracy: 0.8779\n",
      "Epoch 56/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4782 - accuracy: 0.9092 - val_loss: 0.5933 - val_accuracy: 0.8794\n",
      "Epoch 57/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4774 - accuracy: 0.9102 - val_loss: 0.6570 - val_accuracy: 0.8670\n",
      "Epoch 58/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4734 - accuracy: 0.9119 - val_loss: 0.5907 - val_accuracy: 0.8781\n",
      "Epoch 59/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4691 - accuracy: 0.9125 - val_loss: 0.6361 - val_accuracy: 0.8663\n",
      "Epoch 60/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4700 - accuracy: 0.9123 - val_loss: 0.6316 - val_accuracy: 0.8704\n",
      "Epoch 61/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4657 - accuracy: 0.9133 - val_loss: 0.6209 - val_accuracy: 0.8714\n",
      "Epoch 62/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4654 - accuracy: 0.9141 - val_loss: 0.6262 - val_accuracy: 0.8703\n",
      "Epoch 63/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4656 - accuracy: 0.9128 - val_loss: 0.6475 - val_accuracy: 0.8655\n",
      "Epoch 64/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4608 - accuracy: 0.9154 - val_loss: 0.6262 - val_accuracy: 0.8709\n",
      "Epoch 65/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4621 - accuracy: 0.9161 - val_loss: 0.6147 - val_accuracy: 0.8763\n",
      "Epoch 66/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4578 - accuracy: 0.9169 - val_loss: 0.5951 - val_accuracy: 0.8792\n",
      "Epoch 67/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4548 - accuracy: 0.9169 - val_loss: 0.6190 - val_accuracy: 0.8736\n",
      "Epoch 68/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4585 - accuracy: 0.9159 - val_loss: 0.5914 - val_accuracy: 0.8781\n",
      "Epoch 69/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4570 - accuracy: 0.9157 - val_loss: 0.6282 - val_accuracy: 0.8724\n",
      "Epoch 70/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4524 - accuracy: 0.9178 - val_loss: 0.6487 - val_accuracy: 0.8693\n",
      "Epoch 71/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4564 - accuracy: 0.9150 - val_loss: 0.6432 - val_accuracy: 0.8629\n",
      "Epoch 72/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4499 - accuracy: 0.9187 - val_loss: 0.5559 - val_accuracy: 0.8872\n",
      "Epoch 73/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4490 - accuracy: 0.9177 - val_loss: 0.6520 - val_accuracy: 0.8635\n",
      "Epoch 74/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4501 - accuracy: 0.9173 - val_loss: 0.5762 - val_accuracy: 0.8855\n",
      "Epoch 75/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4436 - accuracy: 0.9196 - val_loss: 0.5810 - val_accuracy: 0.8828\n",
      "Epoch 76/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4497 - accuracy: 0.9176 - val_loss: 0.6335 - val_accuracy: 0.8700\n",
      "Epoch 77/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4448 - accuracy: 0.9190 - val_loss: 0.5861 - val_accuracy: 0.8796\n",
      "Epoch 78/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4462 - accuracy: 0.9186 - val_loss: 0.5905 - val_accuracy: 0.8817\n",
      "Epoch 79/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4413 - accuracy: 0.9199 - val_loss: 0.6237 - val_accuracy: 0.8754\n",
      "Epoch 80/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4450 - accuracy: 0.9201 - val_loss: 0.5908 - val_accuracy: 0.8793\n",
      "Epoch 81/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4421 - accuracy: 0.9208 - val_loss: 0.6184 - val_accuracy: 0.8782\n",
      "Epoch 82/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4361 - accuracy: 0.9223 - val_loss: 0.5965 - val_accuracy: 0.8778\n",
      "Epoch 83/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4378 - accuracy: 0.9203 - val_loss: 0.5489 - val_accuracy: 0.8902\n",
      "Epoch 84/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4327 - accuracy: 0.9224 - val_loss: 0.5867 - val_accuracy: 0.8846\n",
      "Epoch 85/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4367 - accuracy: 0.9227 - val_loss: 0.5905 - val_accuracy: 0.8801\n",
      "Epoch 86/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4332 - accuracy: 0.9216 - val_loss: 0.6071 - val_accuracy: 0.8762\n",
      "Epoch 87/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4323 - accuracy: 0.9221 - val_loss: 0.5944 - val_accuracy: 0.8824\n",
      "Epoch 88/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4308 - accuracy: 0.9244 - val_loss: 0.5538 - val_accuracy: 0.8882\n",
      "Epoch 89/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4312 - accuracy: 0.9230 - val_loss: 0.6181 - val_accuracy: 0.8746\n",
      "Epoch 90/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4299 - accuracy: 0.9232 - val_loss: 0.5417 - val_accuracy: 0.8977\n",
      "Epoch 91/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4247 - accuracy: 0.9257 - val_loss: 0.5673 - val_accuracy: 0.8879\n",
      "Epoch 92/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4298 - accuracy: 0.9229 - val_loss: 0.5465 - val_accuracy: 0.8941\n",
      "Epoch 93/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4260 - accuracy: 0.9250 - val_loss: 0.5468 - val_accuracy: 0.8942\n",
      "Epoch 94/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4308 - accuracy: 0.9229 - val_loss: 0.5858 - val_accuracy: 0.8834\n",
      "Epoch 95/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4251 - accuracy: 0.9229 - val_loss: 0.6040 - val_accuracy: 0.8746\n",
      "Epoch 96/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4236 - accuracy: 0.9253 - val_loss: 0.6057 - val_accuracy: 0.8775\n",
      "Epoch 97/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4217 - accuracy: 0.9247 - val_loss: 0.6032 - val_accuracy: 0.8776\n",
      "Epoch 98/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4247 - accuracy: 0.9232 - val_loss: 0.6257 - val_accuracy: 0.8711\n",
      "Epoch 99/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4200 - accuracy: 0.9252 - val_loss: 0.5983 - val_accuracy: 0.8791\n",
      "Epoch 100/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4224 - accuracy: 0.9262 - val_loss: 0.5613 - val_accuracy: 0.8876\n",
      "INFO:tensorflow:Assets written to: models2\\model_3.model\\assets\n",
      "ResNet 4 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "781/781 [==============================] - 74s 95ms/step - loss: 1.8416 - accuracy: 0.4536 - val_loss: 1.6534 - val_accuracy: 0.5336\n",
      "Epoch 2/100\n",
      "781/781 [==============================] - 64s 83ms/step - loss: 1.3015 - accuracy: 0.6315 - val_loss: 1.2329 - val_accuracy: 0.6622\n",
      "Epoch 3/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 1.1066 - accuracy: 0.7015 - val_loss: 1.1406 - val_accuracy: 0.7065\n",
      "Epoch 4/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.9875 - accuracy: 0.7425 - val_loss: 1.1783 - val_accuracy: 0.7026\n",
      "Epoch 5/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.9160 - accuracy: 0.7680 - val_loss: 1.0245 - val_accuracy: 0.7358\n",
      "Epoch 6/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.8639 - accuracy: 0.7846 - val_loss: 0.9584 - val_accuracy: 0.7592\n",
      "Epoch 7/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.8259 - accuracy: 0.7976 - val_loss: 0.8884 - val_accuracy: 0.7862\n",
      "Epoch 8/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7935 - accuracy: 0.8085 - val_loss: 0.8591 - val_accuracy: 0.7900\n",
      "Epoch 9/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7658 - accuracy: 0.8164 - val_loss: 0.8839 - val_accuracy: 0.7828\n",
      "Epoch 10/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7462 - accuracy: 0.8242 - val_loss: 0.8030 - val_accuracy: 0.8043\n",
      "Epoch 11/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7259 - accuracy: 0.8323 - val_loss: 0.8719 - val_accuracy: 0.7972\n",
      "Epoch 12/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7118 - accuracy: 0.8355 - val_loss: 0.7661 - val_accuracy: 0.8199\n",
      "Epoch 13/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6887 - accuracy: 0.8437 - val_loss: 0.7842 - val_accuracy: 0.8142\n",
      "Epoch 14/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6775 - accuracy: 0.8472 - val_loss: 0.7835 - val_accuracy: 0.8197\n",
      "Epoch 15/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6714 - accuracy: 0.8486 - val_loss: 0.7032 - val_accuracy: 0.8418\n",
      "Epoch 16/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6503 - accuracy: 0.8571 - val_loss: 0.8565 - val_accuracy: 0.7938\n",
      "Epoch 17/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6454 - accuracy: 0.8586 - val_loss: 0.6970 - val_accuracy: 0.8434\n",
      "Epoch 18/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6362 - accuracy: 0.8620 - val_loss: 0.6756 - val_accuracy: 0.8534\n",
      "Epoch 19/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6212 - accuracy: 0.8664 - val_loss: 0.7273 - val_accuracy: 0.8321\n",
      "Epoch 20/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6180 - accuracy: 0.8668 - val_loss: 0.7825 - val_accuracy: 0.8193\n",
      "Epoch 21/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6090 - accuracy: 0.8707 - val_loss: 0.6648 - val_accuracy: 0.8545\n",
      "Epoch 22/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5987 - accuracy: 0.8731 - val_loss: 0.6669 - val_accuracy: 0.8526\n",
      "Epoch 23/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5932 - accuracy: 0.8750 - val_loss: 0.7171 - val_accuracy: 0.8374\n",
      "Epoch 24/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5847 - accuracy: 0.8793 - val_loss: 0.6979 - val_accuracy: 0.8446\n",
      "Epoch 25/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5859 - accuracy: 0.8762 - val_loss: 0.7390 - val_accuracy: 0.8358\n",
      "Epoch 26/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5739 - accuracy: 0.8812 - val_loss: 0.6648 - val_accuracy: 0.8543\n",
      "Epoch 27/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5694 - accuracy: 0.8817 - val_loss: 0.6452 - val_accuracy: 0.8607\n",
      "Epoch 28/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5637 - accuracy: 0.8851 - val_loss: 0.6590 - val_accuracy: 0.8560\n",
      "Epoch 29/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5593 - accuracy: 0.8852 - val_loss: 0.7281 - val_accuracy: 0.8394\n",
      "Epoch 30/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5567 - accuracy: 0.8865 - val_loss: 0.7066 - val_accuracy: 0.8456\n",
      "Epoch 31/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5535 - accuracy: 0.8879 - val_loss: 0.7181 - val_accuracy: 0.8450\n",
      "Epoch 32/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5442 - accuracy: 0.8905 - val_loss: 0.7460 - val_accuracy: 0.8452\n",
      "Epoch 33/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5397 - accuracy: 0.8926 - val_loss: 0.6630 - val_accuracy: 0.8551\n",
      "Epoch 34/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5390 - accuracy: 0.8923 - val_loss: 0.7151 - val_accuracy: 0.8422\n",
      "Epoch 35/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5307 - accuracy: 0.8956 - val_loss: 0.6178 - val_accuracy: 0.8722\n",
      "Epoch 36/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5279 - accuracy: 0.8950 - val_loss: 0.6377 - val_accuracy: 0.8687\n",
      "Epoch 37/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5248 - accuracy: 0.8952 - val_loss: 0.7201 - val_accuracy: 0.8442\n",
      "Epoch 38/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5269 - accuracy: 0.8972 - val_loss: 0.6333 - val_accuracy: 0.8659\n",
      "Epoch 39/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5196 - accuracy: 0.8979 - val_loss: 0.6565 - val_accuracy: 0.8588\n",
      "Epoch 40/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5182 - accuracy: 0.8992 - val_loss: 0.6462 - val_accuracy: 0.8587\n",
      "Epoch 41/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5135 - accuracy: 0.9002 - val_loss: 0.6468 - val_accuracy: 0.8637\n",
      "Epoch 42/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5147 - accuracy: 0.9011 - val_loss: 0.7377 - val_accuracy: 0.8382\n",
      "Epoch 43/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5103 - accuracy: 0.9016 - val_loss: 0.6191 - val_accuracy: 0.8718\n",
      "Epoch 44/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5075 - accuracy: 0.9021 - val_loss: 0.6002 - val_accuracy: 0.8757\n",
      "Epoch 45/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5049 - accuracy: 0.9031 - val_loss: 0.6029 - val_accuracy: 0.8770\n",
      "Epoch 46/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5039 - accuracy: 0.9033 - val_loss: 0.6285 - val_accuracy: 0.8642\n",
      "Epoch 47/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4981 - accuracy: 0.9050 - val_loss: 0.5793 - val_accuracy: 0.8872\n",
      "Epoch 48/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4957 - accuracy: 0.9059 - val_loss: 0.5852 - val_accuracy: 0.8785\n",
      "Epoch 49/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4921 - accuracy: 0.9068 - val_loss: 0.6251 - val_accuracy: 0.8683\n",
      "Epoch 50/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4938 - accuracy: 0.9073 - val_loss: 0.6697 - val_accuracy: 0.8583\n",
      "Epoch 51/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4923 - accuracy: 0.9064 - val_loss: 0.5541 - val_accuracy: 0.8897\n",
      "Epoch 52/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4843 - accuracy: 0.9104 - val_loss: 0.5654 - val_accuracy: 0.8893\n",
      "Epoch 53/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4816 - accuracy: 0.9092 - val_loss: 0.6853 - val_accuracy: 0.8558\n",
      "Epoch 54/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4808 - accuracy: 0.9101 - val_loss: 0.6152 - val_accuracy: 0.8726\n",
      "Epoch 55/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4809 - accuracy: 0.9094 - val_loss: 0.6545 - val_accuracy: 0.8644\n",
      "Epoch 56/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4763 - accuracy: 0.9127 - val_loss: 0.6565 - val_accuracy: 0.8611\n",
      "Epoch 57/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4804 - accuracy: 0.9096 - val_loss: 0.5702 - val_accuracy: 0.8897\n",
      "Epoch 58/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4780 - accuracy: 0.9102 - val_loss: 0.6075 - val_accuracy: 0.8762\n",
      "Epoch 59/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4724 - accuracy: 0.9130 - val_loss: 0.6147 - val_accuracy: 0.8729\n",
      "Epoch 60/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4745 - accuracy: 0.9125 - val_loss: 0.6031 - val_accuracy: 0.8782\n",
      "Epoch 61/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4688 - accuracy: 0.9138 - val_loss: 0.5611 - val_accuracy: 0.8894\n",
      "Epoch 62/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4674 - accuracy: 0.9142 - val_loss: 0.5665 - val_accuracy: 0.8826\n",
      "Epoch 63/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4681 - accuracy: 0.9136 - val_loss: 0.5835 - val_accuracy: 0.8796\n",
      "Epoch 64/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4657 - accuracy: 0.9131 - val_loss: 0.5606 - val_accuracy: 0.8861\n",
      "Epoch 65/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4642 - accuracy: 0.9140 - val_loss: 0.6138 - val_accuracy: 0.8751\n",
      "Epoch 66/100\n",
      "781/781 [==============================] - 66s 85ms/step - loss: 0.4683 - accuracy: 0.9132 - val_loss: 0.6127 - val_accuracy: 0.8745\n",
      "Epoch 67/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4597 - accuracy: 0.9165 - val_loss: 0.6007 - val_accuracy: 0.8800\n",
      "Epoch 68/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4616 - accuracy: 0.9160 - val_loss: 0.6564 - val_accuracy: 0.8604\n",
      "Epoch 69/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4597 - accuracy: 0.9159 - val_loss: 0.6217 - val_accuracy: 0.8721\n",
      "Epoch 70/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4589 - accuracy: 0.9171 - val_loss: 0.5594 - val_accuracy: 0.8876\n",
      "Epoch 71/100\n",
      "781/781 [==============================] - 67s 85ms/step - loss: 0.4526 - accuracy: 0.9203 - val_loss: 0.5744 - val_accuracy: 0.8855\n",
      "Epoch 72/100\n",
      "781/781 [==============================] - 67s 85ms/step - loss: 0.4537 - accuracy: 0.9183 - val_loss: 0.6018 - val_accuracy: 0.8805\n",
      "Epoch 73/100\n",
      "781/781 [==============================] - 67s 85ms/step - loss: 0.4542 - accuracy: 0.9176 - val_loss: 0.5816 - val_accuracy: 0.8797\n",
      "Epoch 74/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4520 - accuracy: 0.9193 - val_loss: 0.6845 - val_accuracy: 0.8525\n",
      "Epoch 75/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4525 - accuracy: 0.9182 - val_loss: 0.6079 - val_accuracy: 0.8755\n",
      "Epoch 76/100\n",
      "781/781 [==============================] - 67s 85ms/step - loss: 0.4510 - accuracy: 0.9187 - val_loss: 0.5447 - val_accuracy: 0.8959\n",
      "Epoch 77/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4480 - accuracy: 0.9197 - val_loss: 0.5912 - val_accuracy: 0.8798\n",
      "Epoch 78/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4447 - accuracy: 0.9214 - val_loss: 0.5715 - val_accuracy: 0.8895\n",
      "Epoch 79/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4422 - accuracy: 0.9207 - val_loss: 0.6224 - val_accuracy: 0.8756\n",
      "Epoch 80/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4518 - accuracy: 0.9179 - val_loss: 0.5748 - val_accuracy: 0.8809\n",
      "Epoch 81/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4417 - accuracy: 0.9202 - val_loss: 0.5970 - val_accuracy: 0.8765\n",
      "Epoch 82/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4426 - accuracy: 0.9217 - val_loss: 0.6120 - val_accuracy: 0.8773\n",
      "Epoch 83/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4444 - accuracy: 0.9208 - val_loss: 0.6183 - val_accuracy: 0.8769\n",
      "Epoch 84/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4391 - accuracy: 0.9202 - val_loss: 0.7763 - val_accuracy: 0.8390\n",
      "Epoch 85/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4379 - accuracy: 0.9218 - val_loss: 0.5733 - val_accuracy: 0.8865\n",
      "Epoch 86/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4385 - accuracy: 0.9221 - val_loss: 0.5416 - val_accuracy: 0.8926\n",
      "Epoch 87/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4370 - accuracy: 0.9217 - val_loss: 0.5819 - val_accuracy: 0.8844\n",
      "Epoch 88/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4381 - accuracy: 0.9216 - val_loss: 0.5974 - val_accuracy: 0.8803\n",
      "Epoch 89/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4373 - accuracy: 0.9227 - val_loss: 0.5624 - val_accuracy: 0.8869\n",
      "Epoch 90/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4327 - accuracy: 0.9239 - val_loss: 0.5695 - val_accuracy: 0.8845\n",
      "Epoch 91/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4311 - accuracy: 0.9229 - val_loss: 0.5924 - val_accuracy: 0.8797\n",
      "Epoch 92/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4298 - accuracy: 0.9241 - val_loss: 0.5492 - val_accuracy: 0.8928\n",
      "Epoch 93/100\n",
      "781/781 [==============================] - 66s 84ms/step - loss: 0.4247 - accuracy: 0.9257 - val_loss: 0.5546 - val_accuracy: 0.8878\n",
      "Epoch 94/100\n",
      "781/781 [==============================] - 66s 84ms/step - loss: 0.4307 - accuracy: 0.9228 - val_loss: 0.5753 - val_accuracy: 0.8871\n",
      "Epoch 95/100\n",
      "781/781 [==============================] - 66s 84ms/step - loss: 0.4289 - accuracy: 0.9242 - val_loss: 0.5417 - val_accuracy: 0.8913\n",
      "Epoch 96/100\n",
      "781/781 [==============================] - 66s 84ms/step - loss: 0.4281 - accuracy: 0.9226 - val_loss: 0.5654 - val_accuracy: 0.8886\n",
      "Epoch 97/100\n",
      "781/781 [==============================] - 66s 84ms/step - loss: 0.4269 - accuracy: 0.9251 - val_loss: 0.5671 - val_accuracy: 0.8858\n",
      "Epoch 98/100\n",
      "781/781 [==============================] - 66s 84ms/step - loss: 0.4246 - accuracy: 0.9256 - val_loss: 0.5907 - val_accuracy: 0.8817\n",
      "Epoch 99/100\n",
      "781/781 [==============================] - 66s 84ms/step - loss: 0.4269 - accuracy: 0.9245 - val_loss: 0.5637 - val_accuracy: 0.8903\n",
      "Epoch 100/100\n",
      "781/781 [==============================] - 66s 84ms/step - loss: 0.4244 - accuracy: 0.9254 - val_loss: 0.5350 - val_accuracy: 0.8941\n",
      "INFO:tensorflow:Assets written to: models2\\model_4.model\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "numberOfModels = 2\n",
    "epochs = 100\n",
    "\n",
    "opt = Adam()\n",
    "\n",
    "for i in range(numberOfModels):\n",
    "    print('GoogLeNet', i, 'is being trained...')\n",
    "    \n",
    "    # compile the model\n",
    "    model = MiniGoogLeNet.build(width = 32, height = 32, depth = 3, classes = 10)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])\n",
    "    \n",
    "    # train the model\n",
    "    H = model.fit(aug.flow(trainX, trainY, batch_size = 64), validation_data = (testX, testY), epochs = epochs,\n",
    "                  steps_per_epoch = len(trainX) // 64, verbose = 1)\n",
    "    \n",
    "    # save the model\n",
    "    p = ['models2', 'model_{}.model'.format(i)]\n",
    "    model.save(os.path.sep.join(p))\n",
    "    \n",
    "    # evaluate the network\n",
    "    predictions = model.predict(testX, batch_size=64)\n",
    "    report = classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames)\n",
    "    \n",
    "    # save the classification report to file\n",
    "    p = ['output2', 'model_{}.txt'.format(i)]\n",
    "    f = open(os.path.sep.join(p), 'w')\n",
    "    f.write(report)\n",
    "    f.close()\n",
    "    \n",
    "    # plot the training loss and accuracy\n",
    "    p = ['output2', 'model_{}.png'.format(i)]\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, epochs), H.history['loss'], label = 'train_loss')\n",
    "    plt.plot(np.arange(0, epochs), H.history['val_loss'], label = 'val_loss')\n",
    "    plt.plot(np.arange(0, epochs), H.history['accuracy'], label = 'train_acc')\n",
    "    plt.plot(np.arange(0, epochs), H.history['val_accuracy'], label = 'val_acc')\n",
    "    \n",
    "    # add labels and legend\n",
    "    plt.title('Training Loss and Accuracy for model {}'.format(i))\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Loss/Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # save graphs\n",
    "    plt.savefig(os.path.sep.join(p))\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "for i in range(numberOfModels, 2*numberOfModels+1):\n",
    "    print('ResNet', i, 'is being trained...')\n",
    "    \n",
    "    # compile the model\n",
    "    model = ResNet.build(32, 32, 3, 10, (9, 9, 9), (64, 64, 128, 256), reg=0.0005)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])\n",
    "    \n",
    "    # train the model\n",
    "    H = model.fit(aug.flow(trainX, trainY, batch_size = 64), validation_data = (testX, testY), epochs = epochs,\n",
    "                  steps_per_epoch = len(trainX) // 64, verbose = 1)\n",
    "    \n",
    "    # save the model\n",
    "    p = ['models2', 'model_{}.model'.format(i)]\n",
    "    model.save(os.path.sep.join(p))\n",
    "    \n",
    "    # evaluate the network\n",
    "    predictions = model.predict(testX, batch_size=64)\n",
    "    report = classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames)\n",
    "    \n",
    "    # save the classification report to file\n",
    "    p = ['output2', 'model_{}.txt'.format(i)]\n",
    "    f = open(os.path.sep.join(p), 'w')\n",
    "    f.write(report)\n",
    "    f.close()\n",
    "    \n",
    "    # plot the training loss and accuracy\n",
    "    p = ['output2', 'model_{}.png'.format(i)]\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, epochs), H.history['loss'], label = 'train_loss')\n",
    "    plt.plot(np.arange(0, epochs), H.history['val_loss'], label = 'val_loss')\n",
    "    plt.plot(np.arange(0, epochs), H.history['accuracy'], label = 'train_acc')\n",
    "    plt.plot(np.arange(0, epochs), H.history['val_accuracy'], label = 'val_acc')\n",
    "    \n",
    "    # add labels and legend\n",
    "    plt.title('Training Loss and Accuracy for model {}'.format(i))\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Loss/Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # save graphs\n",
    "    plt.savefig(os.path.sep.join(p))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 1/5\n",
      "Loading model 2/5\n",
      "Loading model 3/5\n",
      "Loading model 4/5\n",
      "Loading model 5/5\n",
      "Evaluating ensemble...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.94      0.95      0.95      1000\n",
      "  automobile       0.95      0.98      0.97      1000\n",
      "        bird       0.91      0.90      0.91      1000\n",
      "         cat       0.85      0.86      0.86      1000\n",
      "        deer       0.93      0.93      0.93      1000\n",
      "         dog       0.92      0.85      0.88      1000\n",
      "        frog       0.90      0.98      0.94      1000\n",
      "       horse       0.97      0.95      0.96      1000\n",
      "        ship       0.97      0.96      0.96      1000\n",
      "       truck       0.97      0.95      0.96      1000\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# construct the path used to collect the models then initialize the\n",
    "# models list\n",
    "modelPaths = os.path.sep.join(['models2', '*.model'])\n",
    "modelPaths = list(glob.glob(modelPaths))\n",
    "models = []\n",
    "\n",
    "# loop over the model paths, loading the model, and adding it to\n",
    "# the list of models\n",
    "for (i, modelPath) in enumerate(modelPaths):\n",
    "\tprint('Loading model {}/{}'.format(i + 1, len(modelPaths)))\n",
    "\tmodels.append(load_model(modelPath))\n",
    "\n",
    "# initialize the list of predictions\n",
    "print('Evaluating ensemble...')\n",
    "predictions = []\n",
    "\n",
    "# loop over the models\n",
    "for model in models:\n",
    "\t# use the current model to make predictions on the testing data,\n",
    "\t# then store these predictions in the aggregate predictions list\n",
    "\tpredictions.append(model.predict(testX, batch_size = 64))\n",
    "\n",
    "# average the probabilities across all model predictions, then show\n",
    "# a classification report\n",
    "predictions = np.average(predictions, axis = 0)\n",
    "print(classification_report(testY.argmax(axis = 1), predictions.argmax(axis = 1), target_names = labelNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Snapshot Ensembles\n",
    "\n",
    "Snapshot ensembles (Huang, et. al., 2017) train a single net in such a way that it repeatedly converges to different local minima, save each state of the model, and use (a subset of) those saved states to build an ensemble. While there may be more risk of this one net landing in similar local minima when training one net compared to training several unrelated nets, it can be dramatically cheaper computationally to ensemble in this way.\n",
    "\n",
    "One way of accomplishing this is to use a cyclic learning rate that starts high and anneals to smaller values until the net settles, saves the state of the model, and then returns to a high learning rate to repeat the cycle. This way, it settles down to a state we will use in the ensemble, but then, a high learning rate will let it jump out of the local minimum it has hopefully reached and continue to a new one. This way, our learning algorithms explore more of the parameter space and capture multiple local minima.\n",
    "\n",
    "The immediate question may be, \"Why not just train a new model from a random initialization each time?\" But, what tends to happen is that most training time occurs when a net is trying to reach its first local minimum. Routing from there to a new one has been shown to be much cheaper under the right circumstances.\n",
    "\n",
    "To accomplish this, we simply need to write a learning rate scheduler that applies a cyclical learning rate of the form (Loschilov and Hutter, 2016):\n",
    "\n",
    "$$\\alpha(t) = f\\left(\\text{mod}\\left(t - 1, \\left\\lceil \\frac{T}{M}\\right\\rceil\\right)\\right),$$\n",
    "\n",
    "where $t$ is the iteration number, $T$ is the total number of training epochs, $M$ is the number of snapshots we will capture, and $f$ is a monotonically decreasing function. So here, we specify how many epochs to train and the number of snapshots we want to take, and then the learning rate cycles every $\\frac{\\text{number of training epochs}}{\\text{number of snapshots}}$ number of epochs.\n",
    "\n",
    "Loschilov and Hutter proposed a shifted cosine function of the form\n",
    "\n",
    "$$\\alpha(t)=\\frac{\\alpha_0}{2}\\left(\\cos\\left(\\frac{\\pi\\text{ mod}(t-1,\\lceil T/M\\rceil)}{\\lceil T/M \\rceil}\\right)+1\\right),$$\n",
    "\n",
    "where $\\alpha_0$ is the initial learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# load cifar10 data\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "labelNames = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", 'dog', \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "# preprocess data\n",
    "trainX = trainX.astype('float')/255.0\n",
    "testX = testX.astype('float')/255.0\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)\n",
    "\n",
    "# create an image generator for data augmentation with random shifting, rotation, and horizontal flips\n",
    "aug = ImageDataGenerator(rotation_range = 10, width_shift_range = 0.1, height_shift_range = 0.1, horizontal_flip = True, fill_mode = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/60\n",
      " 83/781 [==>...........................] - ETA: 6:16 - loss: 3.3040 - accuracy: 0.1431"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-11aa72b02dd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model{epoch:08d}.model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m H = model.fit(aug.flow(trainX, trainY, batch_size = 64), validation_data = (testX, testY), epochs = epochs,\n\u001b[1;32m---> 30\u001b[1;33m               callbacks = callbacks, steps_per_epoch = len(trainX) // 64, verbose = 1)\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# evaluate the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "models = 3\n",
    "initialLearningRate = 0.2\n",
    "epochs = 60\n",
    "\n",
    "# code for a learning rate scheduler\n",
    "def shiftedCosineLearning(epoch):\n",
    "    maxEpochs = epochs\n",
    "    baseLearningRate = initialLearningRate\n",
    "\n",
    "    alpha = (initialLearningRate/2)*(np.cos(np.pi*np.mod(epoch - 1, np.ceil(epochs/models))/np.ceil(epochs/models)) + 1)\n",
    "    \n",
    "    # return the learning rate\n",
    "    return alpha\n",
    "\n",
    "callbacks = [LearningRateScheduler(shiftedCosineLearning)]\n",
    "    \n",
    "# choose the optimizer\n",
    "opt = SGD(lr = initialLearningRate)\n",
    "#pt = Adam()\n",
    "    \n",
    "# compile the model\n",
    "model = MiniVGGNet.build(32, 32, 3, 10)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "    \n",
    "# train the model\n",
    "checkpoint = ModelCheckpoint('model{epoch:08d}.model', period=np.ceil(epochs/models)) \n",
    "H = model.fit(aug.flow(trainX, trainY, batch_size = 64), validation_data = (testX, testY), epochs = epochs,\n",
    "              callbacks = callbacks, steps_per_epoch = len(trainX) // 64, verbose = 1)\n",
    "    \n",
    "# evaluate the network\n",
    "predictions = model.predict(testX, batch_size=64)\n",
    "report = classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we test the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan\\anaconda3\\envs\\DL\\lib\\site-packages\\numpy\\lib\\function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "C:\\Users\\Ryan\\anaconda3\\envs\\DL\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'testY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-110d063a80b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# a classification report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabelNames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'testY' is not defined"
     ]
    }
   ],
   "source": [
    "# THERE IS SOME PREPROCESSING ERROR HERE IN THE LAST STEP...\n",
    "\n",
    "# construct the path used to collect the models then initialize the\n",
    "# models list\n",
    "modelPaths = os.path.sep.join(['*.model'])\n",
    "modelPaths = list(glob.glob(modelPaths))\n",
    "models = []\n",
    "\n",
    "# loop over the model paths, loading the model, and adding it to\n",
    "# the list of models\n",
    "for (i, modelPath) in enumerate(modelPaths):\n",
    "\tprint('Loading model {}/{}'.format(i + 1, len(modelPaths)))\n",
    "\tmodels.append(load_model(modelPath))\n",
    "\n",
    "# initialize the list of predictions\n",
    "print('Evaluating ensemble...')\n",
    "predictions = []\n",
    "\n",
    "# loop over the models\n",
    "for model in models:\n",
    "\t# use the current model to make predictions on the testing data,\n",
    "\t# then store these predictions in the aggregate predictions list\n",
    "\tpredictions.append(model.predict(testX, batch_size=64))\n",
    "\n",
    "# average the probabilities across all model predictions, then show\n",
    "# a classification report\n",
    "predictions = np.average(predictions, axis=0)\n",
    "print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Transfer learning is where we take nets pre-trained on huge datasets like ImageNet, load the weights, and use those as starting points for training on a new dataset. The idea is that knowledge learning about one dataset, if it is somewhat related to your dataset, can *transfer* to knowledge of your dataset with only partial training.\n",
    "\n",
    "For example, a neural net that is effective at classifying words spoken in English might be effective for learning words spoken in Spanish without totally starting from scratch with training and hyperparameter tuning. After all, both languages have roughly the same alphabet and many similarities in pronunciation of letters.\n",
    "\n",
    "One approach takes a pre-trained convolutional neural net containing all the parameters successful at classifying the intended dataset, remove the fully-connected layers at the end, and feed your *different* dataset through the net to the end of the last pooling layer. Then, treat the outputs from each input as a new dataset that has been preprocessed by ths pre-trained CNN. Lastly, apply another classifier to this dataset.\n",
    "\n",
    "Another common approach is to take the same kind of pre-trained CNN and re-initialize the weights of the fully-connected layers at the end. Then, \"freeze\" all the parameters before the fully-connected layers. Last, train the parameters of the last few layers with a small learning rate on the new dataset, but with one caveat: as backpropagation moves backward through the network determining weight updates, it stops when it reaches the frozen layers and does not adjust those weights at all.\n",
    "\n",
    "Let's learn how to implement both approaches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF5\n",
    "\n",
    "If we want to work with huge pre-trained neural nets like VGG19 or other deep CNNs, storing them takes far more space than our RAM is likely to support, so we need to store them on HDD/SDDs in an efficient way. Keras's model format is pretty large, but HDF5 is a good data format for this, but we need some code to be able to interface with this format, which we write below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5DatasetWriter:\n",
    "    def __init__(self, dims, outputPath, dataKey = 'images', bufferSize = 1000):\n",
    "        # check if outputpath exists\n",
    "        if os.path.exists(outputPath):\n",
    "            raise ValueError('The supplied `outputPath` already exists and cannot be overwritten. Delete '\n",
    "                            ' the file manually before continuing.', outputPath)\n",
    "            \n",
    "        # open the HDF5 database for writing and create two datasets: one to store the images/features and\n",
    "        # one to store the labels\n",
    "        self.db = h5py.File(outputPath, 'w')\n",
    "        self.data = self.db.create_dataset(dataKey, dims, dtype = 'float')\n",
    "        self.labels = self.db.create_dataset('labels', (dims[0],), dtype = 'float')\n",
    "        \n",
    "        # store the buffer size and initialize the buffer and index\n",
    "        self.bufferSize = bufferSize\n",
    "        self.buffer = {'data': [], 'labels': []}\n",
    "        self.index = 0\n",
    "        \n",
    "    def add(self, rows, labels):\n",
    "        # add the rows and labels to the buffer\n",
    "        self.buffer['data'].extend(rows)\n",
    "        self.buffer['labels'].extend(labels)\n",
    "        \n",
    "        # check if the buffer needs to be flushed to disk\n",
    "        if len(self.buffer['data']) >= self.bufferSize:\n",
    "            self.flush()\n",
    "            \n",
    "    def flush(self):\n",
    "        # write the buffer to disk and reset buffer\n",
    "        i = self.index + len(self.buffer['data'])\n",
    "        self.data[self.index:i] = self.buffer['data']\n",
    "        self.labels[self.index:i] = self.buffer['labels']\n",
    "        \n",
    "        self.index = i\n",
    "        self.buffer = {'data': [], 'labels': []}\n",
    "        \n",
    "    def storeClassLabels(self, classLabels):\n",
    "        # create a dataset to store class label names, then store them\n",
    "        dt = h5py.special_dtype(vlen = str)\n",
    "        labelSet = self.db.create_dataset('label_names', (len(classLabels),), dtype = dt)\n",
    "        labelSet[:] = classLabels\n",
    "        \n",
    "    def close(self):\n",
    "        # flush entries to disk if needed\n",
    "        if len(self.buffer['data']) > 0:\n",
    "            self.flush()\n",
    "            \n",
    "        # close the dataset\n",
    "        self.db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Let's write some code to extract features from an arbitrary image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(batch_size, dataset, output, buffer_size = 1000):\n",
    "\n",
    "    bs = batch_size\n",
    "    \n",
    "    # grab the list of images that we'll be describing then randomly\n",
    "    # shuffle them to allow for easy training and testing splits via\n",
    "    # array slicing during training time\n",
    "    print(\"[INFO] loading images...\")\n",
    "    imagePaths = list(paths.list_images(dataset))\n",
    "    random.shuffle(imagePaths)\n",
    "\n",
    "    # extract the class labels from the image paths then encode the\n",
    "    # labels\n",
    "    labels = [p.split(os.path.sep)[-2] for p in imagePaths]\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(labels)\n",
    "\n",
    "    # load the VGG16 network\n",
    "    print(\"[INFO] loading network...\")\n",
    "    model = VGG16(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "    # initialize the HDF5 dataset writer, then store the class label\n",
    "    # names in the dataset\n",
    "    dataset = HDF5DatasetWriter((len(imagePaths), 512 * 7 * 7),\n",
    "        output, dataKey=\"features\", bufferSize=buffer_size)\n",
    "    dataset.storeClassLabels(le.classes_)\n",
    "\n",
    "    # initialize the progress bar\n",
    "    widgets = [\"Extracting Features: \", progressbar.Percentage(), \" \",\n",
    "        progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "    pbar = progressbar.ProgressBar(maxval=len(imagePaths),\n",
    "        widgets=widgets).start()\n",
    "\n",
    "    # loop over the images in batches\n",
    "    for i in np.arange(0, len(imagePaths), bs):\n",
    "        # extract the batch of images and labels, then initialize the\n",
    "        # list of actual images that will be passed through the network\n",
    "        # for feature extraction\n",
    "        batchPaths = imagePaths[i:i + bs]\n",
    "        batchLabels = labels[i:i + bs]\n",
    "        batchImages = []\n",
    "\n",
    "        # loop over the images and labels in the current batch\n",
    "        for (j, imagePath) in enumerate(batchPaths):\n",
    "            # load the input image using the Keras helper utility\n",
    "            # while ensuring the image is resized to 224x224 pixels\n",
    "            image = load_img(imagePath, target_size=(224, 224))\n",
    "            image = img_to_array(image)\n",
    "\n",
    "            # preprocess the image by (1) expanding the dimensions and\n",
    "            # (2) subtracting the mean RGB pixel intensity from the\n",
    "            # ImageNet dataset\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            image = imagenet_utils.preprocess_input(image)\n",
    "\n",
    "            # add the image to the batch\n",
    "            batchImages.append(image)\n",
    "\n",
    "        # pass the images through the network and use the outputs as\n",
    "        # our actual features\n",
    "        batchImages = np.vstack(batchImages)\n",
    "        features = model.predict(batchImages, batch_size=bs)\n",
    "\n",
    "        # reshape the features so that each image is represented by\n",
    "        # a flattened feature vector of the `MaxPooling2D` outputs\n",
    "        features = features.reshape((features.shape[0], 512 * 7 * 7))\n",
    "\n",
    "        # add the features and labels to our HDF5 dataset\n",
    "        dataset.add(features, batchLabels)\n",
    "        pbar.update(i)\n",
    "\n",
    "    # close the dataset\n",
    "    dataset.close()\n",
    "    pbar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Features from Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n",
      "[INFO] loading network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100% |#####################################| Time: 0:00:32\n"
     ]
    }
   ],
   "source": [
    "extractFeatures(32, '../datasets/animals/images', '../datasets/animals/hdf5/features.hdf5', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n",
      "[INFO] loading network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100% |#####################################| Time: 0:01:37\n"
     ]
    }
   ],
   "source": [
    "extractFeatures(32, '../datasets/caltech-101/images', '../datasets/caltech-101/hdf5/features.hdf5', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n",
      "[INFO] loading network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100% |#####################################| Time: 0:00:16\n"
     ]
    }
   ],
   "source": [
    "extractFeatures(32, '../datasets/flowers17/images', '../datasets/flowers17/hdf5/features.hdf5', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Classifier on Extracted Features\n",
    "\n",
    "We have used a VGG16 net pre-trained on ImageNet and used it to extract features from three *different* datasets. We will now train a simple classifier on this new dataset of features extracted by the VGG16 net and see if the learning can actually be transferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = h5py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
