{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "We have more or less exhausted the basics of customizing a fully-connected feedforward neural network. We know how to do lots of things that are important. We can:\n",
    "\n",
    "* Normalize the data\n",
    "* Initialize the weights and biases\n",
    "* Customize network architectures\n",
    "* Use several activation functions\n",
    "* Learn with gradient descent (or better, SGD) and backpropagation\n",
    "* Modify SGD with momentum and other methods\n",
    "* Use several loss functions\n",
    "* Apply regularization methods\n",
    "\n",
    "With all of this, we can make them quite good at some tasks, such as identifying handwritten digits from the MNIST dataset, by using carefully tuning the hyperparameters.\n",
    "\n",
    "Harder problems, like classifying images from the CIFAR-10 dataset, are still a challenge. Classification accuracy using these neural nets is significantly better on this dataset (about 55% accuracy) than simplistic methods like nearest neighbors (about 25% accuracy). However, 55% accuracy is not very reliable practically.\n",
    "\n",
    "There are a number of options that might help:\n",
    "\n",
    "* Use deeper networks\n",
    "* Use wider networks\n",
    "* Use higher dimensional data\n",
    "\n",
    "All of these options have something in common: they are computationally expensive due to the increasing dimensionality of the data and/or the weight matrices, so they are not particularly helpful options. A theme of the next portion of the class will be to try to manage this dimensionality problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components Analysis (PCA)\n",
    "\n",
    "Principal components analysis (PCA) is a method for reducing the dimension of the data by using some idea from linear algebra to map the rows from a data matrix $X$ from its default $d$-dimensional space to an $r$-dimensional space for some $r < d$.\n",
    "\n",
    "Please see the notes in class for the math that makes this work, but the code is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Principal component analysis\n",
    "def PCA(D, alpha):    \n",
    "    print('[INFO] Now running principal components analysis (PCA)')\n",
    "    \n",
    "    # find the original dimension and print it\n",
    "    dimension = D.shape[1]\n",
    "    print('[INFO] The original dimension of the data is', dimension)\n",
    "    \n",
    "    # center D to have mean 0\n",
    "    D -= np.mean(D, axis=0)\n",
    "    \n",
    "    # compute the covariance matrix\n",
    "    Sigma = (1/D.shape[0]) * D.T @ D\n",
    "\n",
    "    # compute the eigenvalues and eigenvectors of D^T D\n",
    "    (eValues, eVectors) = np.linalg.eigh(Sigma)\n",
    "\n",
    "    # compute the total variance\n",
    "    varD = np.sum(eValues)\n",
    "    \n",
    "    # initialize the variance for A to 0\n",
    "    varA = 0.0\n",
    "    \n",
    "    # reverse eValues and eVectors\n",
    "    eValues = np.flip(eValues)\n",
    "    eVectors = np.flip(eVectors, axis=1)\n",
    "\n",
    "    # find the minimum dimension consisting of fraction at least alpha of the total variance\n",
    "    for r in np.arange(0, eValues.shape[0]):\n",
    "        varA += eValues[r]\n",
    "        ratio = varA/varD\n",
    "\n",
    "        if ratio > alpha:\n",
    "            dimension = r + 1\n",
    "            print('[INFO] The new dimension of the data is', dimension, 'and it explains', ratio, 'of the variance')\n",
    "            break\n",
    "            \n",
    "    # construct the new basis\n",
    "    basis = eVectors[:,:dimension]\n",
    "    \n",
    "    # create the datapoints in the new space\n",
    "    A = D @ basis\n",
    "    \n",
    "    # return points\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this out a bit, let's bring in our neural network code and our favorite dataset (MNIST), use PCA to reduce the dimension of MNIST, and then train the neural net to classify the lower-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# This class is for a fully-connected feedforward neural network using stochastic gradient descent\n",
    "#\n",
    "# Inputs:\n",
    "#\n",
    "#   layers - a list of numbers of neurons in each layer\n",
    "#\n",
    "#   alpha - learning rate\n",
    "#\n",
    "#   annealing - a list specifying the annealing schedule for the learning rate\n",
    "#     'none' - use a constant learning rate\n",
    "#     ['step', k] - decay the learning rate to 0 at k evenly spaced times with identical steps\n",
    "#     ['exp', k] - decay the learning rate exponentially alpha = alpha0(exp(-k*epoch))\n",
    "#     ['inv', k] - decay the learning rate as alpha = alpha0/(1 + k*epoch)\n",
    "#\n",
    "#   lambda1 - L1 penalty coefficient (positive)\n",
    "#\n",
    "#   lambda2 - L2 penalty coefficient (positive)\n",
    "#\n",
    "#   batchSize - size of mini-batches for SGD\n",
    "#\n",
    "#   gamma - momentum coefficient (between 0 and 1)\n",
    "#\n",
    "#   initialization - a list specifying the initalization\n",
    "#     ['normal', k] - standard normal random variables divided by k\n",
    "#     ['uniform', a, b] - uniform random variables between a and b\n",
    "#     'Nielsen' - standard normal random variables divided by the number of nodes in the next layer\n",
    "#     ['LeCun', 'uniform' or 'normal'] - normal or uniform random variables centered at 0 with standard deviation sqrt(3/nodes in previous\n",
    "#                                        layer) or lower/upper bounds of +/- the same, respectively\n",
    "#     ['Glorot', 'uniform' or 'normal'] - normal or uniform random variables centered at 0 with standard deviation sqrt(1/mean of nodes in\n",
    "#                                         previous and next layers) or lower/upper bounds of +/- the same, respectively\n",
    "#     ['He', 'uniform' or 'normal'] - normal or uniform random variables centered at 0 with standard deviation sqrt(6/nodes in previous\n",
    "#                                     layer) or lower/upper bounds of +/- the same, respectively\n",
    "#\n",
    "#   activations - string to choose which activation functions to use:\n",
    "#     'sigmoid' - sigmoid function\n",
    "#     'ReLU' - rectified linear unit\n",
    "#     'ELU' - exponential linear unit\n",
    "#\n",
    "#   loss - string to choose which loss function to use:\n",
    "#     'sum-of-squares'\n",
    "#     'cross-entropy'\n",
    "\n",
    "class FeedforwardNeuralNetwork:\n",
    "    \n",
    "    def __init__(self, layers, alpha = 0.1, annealing = 'none', lambda1 = 0, lambda2 = 0, batchSize = 32, gamma = 0.0,\n",
    "                 initialization = 'normal', activations = 'sigmoid', loss = 'sum-of-squares'):\n",
    "        # list of weight matrices between layers\n",
    "        self.W = []\n",
    "        \n",
    "        # network architecture will be a vector of numbers of nodes for each layer\n",
    "        self.layers = layers\n",
    "        \n",
    "        # learning rate\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # annealing policy\n",
    "        self.annealing = annealing\n",
    "        \n",
    "        # L1 penalty coefficient\n",
    "        self.lambda1 = lambda1\n",
    "        \n",
    "        # L2 penalty coefficient\n",
    "        self.lambda2 = lambda2\n",
    "        \n",
    "        # batch size\n",
    "        self.batchSize = batchSize\n",
    "        \n",
    "        # momentum parameter\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # activation type\n",
    "        self.activation = activations\n",
    "        \n",
    "        # loss function\n",
    "        self.loss = loss\n",
    "        \n",
    "        # initialize the weights -- this is our initial guess for gradient descent\n",
    "        \n",
    "        # initialize the weights between layers (up to the next-to-last one) as normal random variables\n",
    "        if initialization[0] == 'normal':\n",
    "            for i in np.arange(0, len(layers) - 2):\n",
    "                self.W.append(np.random.normal(0.0, 1.0, size=(layers[i] + 1, layers[i + 1] + 1))/initialization[1])\n",
    "\n",
    "            # initialize weights between the last two layers (we don't want bias for the last one)\n",
    "            self.W.append(np.random.normal(0.0, 1.0, size=(layers[-2] + 1, layers[-1]))/initialization[1])\n",
    "            \n",
    "        if initialization[0] == 'uniform':\n",
    "            # initialize the weights between layers (up to the next-to-last one) as normal random variables\n",
    "            for i in np.arange(0, len(layers) - 2):\n",
    "                self.W.append(np.random.uniform(initialization[1], initialization[2], size=(layers[i] + 1, layers[i + 1] + 1)))\n",
    "\n",
    "            # initialize weights between the last two layers (we don't want bias for the last one)\n",
    "            self.W.append(np.random.uniform(initialization[1], initialization[2], size=(layers[-2] + 1, layers[-1])))\n",
    "            \n",
    "        if initialization == 'Nielsen':\n",
    "            # initialize the weights between layers (up to the next-to-last one) as normal random variables\n",
    "            for i in np.arange(0, len(layers) - 2):\n",
    "                self.W.append(np.random.normal(0.0, 1.0, size=(layers[i] + 1, layers[i + 1] + 1))/layers[i+1])\n",
    "\n",
    "            # initialize weights between the last two layers (we don't want bias for the last one)\n",
    "            self.W.append(np.random.normal(0.0, 1.0, size=(layers[-2] + 1, layers[-1]))/layers[-1])\n",
    "            \n",
    "        # LeCun, Glorot, and He initialization\n",
    "        if initialization[0] in ['LeCun', 'Glorot', 'He']:\n",
    "            # initialize the weights between layers (up to the next-to-last one) as normal random variables\n",
    "            for i in np.arange(0, len(layers) - 2):\n",
    "                \n",
    "                # define the limit term for normal random variables\n",
    "                \n",
    "                # LeCun initialization (\"efficient backprop\", default initialization in PyTorch)\n",
    "                if initialization[0] == 'LeCun':\n",
    "                    limit = np.sqrt(1.0 / layers[i+1])\n",
    "                    \n",
    "                # Glorot initialization (default initialization in Keras)\n",
    "                elif initialization[0] == 'Glorot':\n",
    "                    mean = (layers[i] + layers[i+1])/2.0\n",
    "                    limit = np.sqrt(1.0 / mean)\n",
    "                    \n",
    "                # He initialization (typically used for very deep nets with PReLU activation)\n",
    "                elif initialization[0] == 'He':\n",
    "                    limit = np.sqrt(2.0 / layers[i+1])\n",
    "                \n",
    "                # generate the weights\n",
    "                if initialization[1] == 'normal':\n",
    "                    self.W.append(np.random.normal(0.0, limit, size=(layers[i] + 1, layers[i + 1] + 1)))\n",
    "                    \n",
    "                elif initialization[1] == 'uniform':\n",
    "                    limit *= np.sqrt(3.0)\n",
    "                    self.W.append(np.random.uniform(-limit, limit, size=(layers[i] + 1, layers[i + 1] + 1)))\n",
    "\n",
    "            # initialize weights between the last two layers (we don't want bias for the last one)\n",
    "\n",
    "            # define the limit term for normal random variables\n",
    "            if initialization[0] == 'LeCun':\n",
    "                limit = np.sqrt(1.0 / layers[-2])\n",
    "                \n",
    "            elif initialization[0] == 'Glorot':\n",
    "                mean = (layers[-2] + layers[-1])/2.0\n",
    "                limit = np.sqrt(1.0 / mean)\n",
    "                    \n",
    "            elif initialization[0] == 'He':\n",
    "                limit = np.sqrt(2.0 / layers[-2])\n",
    "            \n",
    "            # initialize the weights\n",
    "            if initialization[1] == 'normal':\n",
    "                self.W.append(np.random.normal(0.0, limit, size=(layers[-2] + 1, layers[-1])))\n",
    "                \n",
    "            elif initialization[1] == 'uniform':\n",
    "                limit *= np.sqrt(3.0)\n",
    "                self.W.append(np.random.uniform(-limit, limit, size=(layers[-2] + 1, layers[-1])))\n",
    "        \n",
    "    # define the activation function\n",
    "    def activate(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "        if self.activation == 'ReLU':\n",
    "            return x*(x >= 0)\n",
    "        \n",
    "        if self.activation == 'ELU':\n",
    "            return x*(x >= 0) + 0.1*(np.exp(x) - 1)*(x < 0)\n",
    "    \n",
    "    # define the activation derivative (where x is the INPUT to activation function)\n",
    "    def activationDerivative(self, x):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return self.activate(x) * (1 - self.activate(x))\n",
    "        \n",
    "        if self.activation == 'ReLU':\n",
    "            return x >= 0\n",
    "        \n",
    "        if self.activation == 'ELU':\n",
    "            return 1*(x >= 0) + 0.1*np.exp(x)*(x < 0)\n",
    "    \n",
    "    def getNextBatch(self, X, y, batchSize):\n",
    "        for i in np.arange(0, X.shape[0], batchSize):\n",
    "            yield (X[i:i + batchSize], y[i:i + batchSize])\n",
    "    \n",
    "    # fit the model\n",
    "    def fit(self, X, y, valX, valY, epochs = 10000, update = 1000, printPerformance = True):\n",
    "        # add a column of ones to the end of X\n",
    "        X = np.hstack((X, np.ones([X.shape[0],1])))\n",
    "        losses = []\n",
    "        trainAccuracies = []\n",
    "        validationAccuracies = []\n",
    "        trainY = y\n",
    "        \n",
    "        numberOfExamples = X.shape[0]\n",
    "        \n",
    "        # use one-hot encoding for the training labels\n",
    "        y = LabelBinarizer().fit_transform(y)\n",
    "        \n",
    "        v = []\n",
    "        for layer in np.arange(0,len(self.W)):\n",
    "            v.append(np.zeros(self.W[layer].shape))\n",
    "            \n",
    "        alpha = self.alpha\n",
    "\n",
    "        for epoch in np.arange(0,epochs):\n",
    "            \n",
    "            if self.annealing[0] == 'step':\n",
    "                stepSize = self.alpha/self.annealing[1]\n",
    "                if epochs > 0 and epoch % self.annealing[1] == 0:\n",
    "                    alpha -= stepSize\n",
    "                    print(alpha)\n",
    "                \n",
    "            elif self.annealing[0] == 'exp':\n",
    "                alpha = self.alpha * np.exp(-self.annealing[1] * epoch)\n",
    "                    \n",
    "            elif self.annealing[0] == 'inv':\n",
    "                alpha = self.alpha / (1 + self.annealing[1] * epoch)\n",
    "            \n",
    "            # randomize the examples\n",
    "            p = np.arange(0,X.shape[0])\n",
    "            np.random.shuffle(p)\n",
    "            X = X[p]\n",
    "            y = y[p]\n",
    "\n",
    "            # feed forward, backprop, and weight update\n",
    "            for (x, target) in self.getNextBatch(X, y, self.batchSize):\n",
    "                # make a list of output activations from the first layer\n",
    "                # (just the original x values)\n",
    "                A = [np.atleast_2d(x)]\n",
    "                Z = [np.atleast_2d(x)]\n",
    "                \n",
    "                # feed forward\n",
    "                for layer in np.arange(0, len(self.W)):\n",
    "                    \n",
    "                    # feed through one layer and apply sigmoid activation\n",
    "                    net = A[layer].dot(self.W[layer])\n",
    "                    out = self.activate(net)\n",
    "                    \n",
    "                    # add our network output to the list of activations\n",
    "                    A.append(out)\n",
    "                    Z.append(net)\n",
    "                    \n",
    "                # backpropagation (coming soon!)\n",
    "                error = A[-1] - target\n",
    "                \n",
    "                if self.loss == 'sum-of-squares':\n",
    "                    D = [error * self.activationDerivative(Z[-1])]\n",
    "                    \n",
    "                if self.loss == 'cross-entropy':\n",
    "                    D = [error]\n",
    "                \n",
    "                # loop backwards over the layers to build up deltas\n",
    "                for layer in np.arange(len(A) - 2, 0, -1):\n",
    "                    delta = D[-1].dot(self.W[layer].T)\n",
    "                    delta = delta * self.activationDerivative(Z[layer])\n",
    "                    D.append(delta)\n",
    "                    \n",
    "                # reverse the deltas since we looped in reverse\n",
    "                D = D[::-1]\n",
    "                \n",
    "                # weight update\n",
    "                for layer in np.arange(0, len(self.W)):\n",
    "                    gradient = (A[layer].T.dot(D[layer])\n",
    "                                + (self.lambda1 / numberOfExamples) * np.sign(self.W[layer])\n",
    "                                + (self.lambda2 / numberOfExamples) * self.W[layer])\n",
    "                    \n",
    "                    v[layer] = self.gamma * v[layer] + alpha * gradient\n",
    "                    self.W[layer] -= v[layer]\n",
    "              \n",
    "            # print the loss (or maybe more) each 'update' number of epochs\n",
    "            if (epoch + 1) % update == 0:\n",
    "                #loss = self.computeLoss(X,y)\n",
    "\n",
    "                # if we chose, compute the accuracy (this makes it run slower)\n",
    "                if printPerformance:\n",
    "                    predictedY = self.predict(valX)\n",
    "                    predictedY = predictedY.argmax(axis=1)\n",
    "                    validationAccuracy = accuracy_score(valY, predictedY)           \n",
    "\n",
    "                    predictedY = self.predict(trainX)\n",
    "                    predictedY = predictedY.argmax(axis=1)\n",
    "                    trainAccuracy = accuracy_score(trainY, predictedY)\n",
    "\n",
    "                    validationAccuracies.append(validationAccuracy)\n",
    "                    trainAccuracies.append(trainAccuracy)\n",
    "                    losses.append(loss)\n",
    "                    print(\"[INFO] epoch = {}, loss = {:.6f}, training accuracy = {:.6f}, validation accuracy = {:.6f}\".format(epoch + 1, loss, trainAccuracy, testAccuracy))\n",
    "                    \n",
    "                # otherwise, simply print the training loss\n",
    "                else:\n",
    "                    #losses.append(loss)\n",
    "                    #print(\"[INFO] epoch = {}, loss = {:.6f}\".format(epoch + 1, loss))\n",
    "                    print('[INFO] Epoch =', epoch + 1, 'of', epochs)\n",
    "\n",
    "        # if we chose to print the performance, plot loss, training accuracy, and validation accuracy for each epoch\n",
    "        if printPerformance:\n",
    "            fig, ax1 = plt.subplots()\n",
    "\n",
    "            # plot the losses\n",
    "            p1 = ax1.plot(np.arange(0, epochs, update), losses, label = 'Loss')\n",
    "            ax1.set_xlabel('Training Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "\n",
    "            # make another y axis using the same x axis\n",
    "            ax2 = ax1.twinx()\n",
    "            \n",
    "            # plot the accuracy\n",
    "            p2 = ax2.plot(np.arange(0, epochs, update), trainAccuracies, label = 'Accuracy (train)', color = 'tab:orange')\n",
    "            p3 = ax2.plot(np.arange(0, epochs, update), validationAccuracies, label = 'Accuracy (validation)', color = 'tab:green')\n",
    "            ax2.set_ylabel('Accuracy')\n",
    "\n",
    "            # add a legend\n",
    "            ps = p1 + p2 + p3\n",
    "            labs = [p.get_label() for p in ps]\n",
    "            ax1.legend(ps, labs, loc=0)\n",
    "            \n",
    "    # feed data into the network and compute the outputs\n",
    "    def predict(self, X, addOnes = True):\n",
    "        # initialize data, be sure it's the right dimension\n",
    "        p = np.atleast_2d(X)\n",
    "        \n",
    "        # add a column of 1s for bias\n",
    "        if addOnes:\n",
    "            p = np.hstack((p, np.ones([X.shape[0],1])))\n",
    "        \n",
    "        # feed forward!\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            p = self.activate(np.dot(p, self.W[layer]))\n",
    "            \n",
    "        return p\n",
    "    \n",
    "    # compute the loss function\n",
    "    def computeLoss(self, X, y):\n",
    "        # initialize data, be sure it's the right dimension\n",
    "        y = np.atleast_2d(y)\n",
    "        \n",
    "        # feed the datapoints through the network to get predicted outputs\n",
    "        predictions = self.predict(X, addOnes = False)\n",
    "        \n",
    "        # if the loss function is sum of squares, compute it\n",
    "        if self.loss == 'sum-of-squares':\n",
    "            loss = np.sum((predictions - y)**2) / 2.0\n",
    "            \n",
    "        # if the loss function is cross-entropy, compute it\n",
    "        if self.loss == 'cross-entropy':\n",
    "            loss = np.sum(np.nan_to_num(-y*np.log(predictions)-(1-y)*np.log(1-predictions)))\n",
    "            \n",
    "        # if there is an L1 penalty, compute it and add it to the loss\n",
    "        if self.lambda1 != 0:\n",
    "            # compute the L1 penalty \n",
    "            L1penalty = 0\n",
    "\n",
    "            for layer in np.arange(0,len(self.W)):\n",
    "                L1penalty += np.sum(np.abs(self.W[layer]))\n",
    "        \n",
    "            # add the L1 penalty to the loss\n",
    "            loss = loss + (self.lambda1 / X.shape[0]) * L1penalty\n",
    "            \n",
    "        # if there is an L2 penalty, compute it and add it to the loss\n",
    "        if self.lambda2 != 0:\n",
    "            # compute the L2 penalty \n",
    "            L2penalty = 0\n",
    "\n",
    "            for layer in np.arange(0,len(self.W)):\n",
    "                L2penalty += np.sum(self.W[layer] ** 2)\n",
    "        \n",
    "            # add the L2 penalty to the loss\n",
    "            loss = loss + (self.lambda2 / X.shape[0]) * L2penalty\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's run it on 60000 MNIST images and compute the time. Later, we will compare it to using PCA and training the net on the resulting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Now training the neural network\n",
      "[INFO] Epoch = 1 of 25\n",
      "[INFO] Epoch = 2 of 25\n",
      "[INFO] Epoch = 3 of 25\n",
      "[INFO] Epoch = 4 of 25\n",
      "[INFO] Epoch = 5 of 25\n",
      "[INFO] Epoch = 6 of 25\n",
      "[INFO] Epoch = 7 of 25\n",
      "[INFO] Epoch = 8 of 25\n",
      "[INFO] Epoch = 9 of 25\n",
      "[INFO] Epoch = 10 of 25\n",
      "[INFO] Epoch = 11 of 25\n",
      "[INFO] Epoch = 12 of 25\n",
      "[INFO] Epoch = 13 of 25\n",
      "[INFO] Epoch = 14 of 25\n",
      "[INFO] Epoch = 15 of 25\n",
      "[INFO] Epoch = 16 of 25\n",
      "[INFO] Epoch = 17 of 25\n",
      "[INFO] Epoch = 18 of 25\n",
      "[INFO] Epoch = 19 of 25\n",
      "[INFO] Epoch = 20 of 25\n",
      "[INFO] Epoch = 21 of 25\n",
      "[INFO] Epoch = 22 of 25\n",
      "[INFO] Epoch = 23 of 25\n",
      "[INFO] Epoch = 24 of 25\n",
      "[INFO] Epoch = 25 of 25\n",
      "[INFO] Neural network training is finished after 39.66689205169678 seconds\n",
      "[INFO] Training set accuracy is 0.9955555555555555\n",
      "[INFO] Validation set accuracy is 0.9816666666666667\n"
     ]
    }
   ],
   "source": [
    "numberOfImages = 60000\n",
    "\n",
    "# The datapoints are in mnistData[0][0]\n",
    "X = data[0][0][:numberOfImages].reshape([numberOfImages,28*28]).astype('float')\n",
    "\n",
    "# Normalize the data\n",
    "X = X/255.0\n",
    "\n",
    "# The labels are in mnistData[0][1]\n",
    "Y = data[0][1][:numberOfImages]\n",
    "\n",
    "# randomly choose 75% of the data to be the traini(ng set and 25% for the validation set\n",
    "(trainX, valX, trainY, valY) = train_test_split(X, Y, test_size = 0.25)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# fit the model to the training data\n",
    "print('[INFO] Now training the neural network')\n",
    "\n",
    "model = FeedforwardNeuralNetwork([X.shape[1], 128, 10], 0.001, ['exp', 0.1], 20.0, 300.0, 64, 0.9,\n",
    "                                 ['Glorot', 'uniform'], 'ReLU', 'cross-entropy')\n",
    "\n",
    "model.fit(trainX, trainY, valX, valY, 25, 1, False)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('[INFO] Neural network training is finished after', end - start, 'seconds')\n",
    "\n",
    "# Compute the outputs and measure the accuracy on the train and validation sets\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "predictedY = model.predict(trainX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "trainY = trainY.argmax(axis=1)\n",
    "print('[INFO] Training set accuracy is', accuracy_score(trainY, predictedY))\n",
    "\n",
    "valY = LabelBinarizer().fit_transform(valY)\n",
    "predictedY = model.predict(valX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "valY = valY.argmax(axis=1)\n",
    "print('[INFO] Validation set accuracy is', accuracy_score(valY, predictedY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved 98% accuracy on the validation set and the run time was 40 seconds. Now, let's do exactly the same thing *except* we will use PCA to reduce the dimension of the same data before running the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Now running principal components analysis (PCA)\n",
      "[INFO] The original dimension of the data is 784\n",
      "[INFO] The new dimension of the data is 44 and it explains 0.80329075538368 of the variance\n",
      "[INFO] PCA finished in 3.970233201980591 seconds\n",
      "[INFO] Now training the neural network\n",
      "[INFO] Epoch = 1 of 25\n",
      "[INFO] Epoch = 2 of 25\n",
      "[INFO] Epoch = 3 of 25\n",
      "[INFO] Epoch = 4 of 25\n",
      "[INFO] Epoch = 5 of 25\n",
      "[INFO] Epoch = 6 of 25\n",
      "[INFO] Epoch = 7 of 25\n",
      "[INFO] Epoch = 8 of 25\n",
      "[INFO] Epoch = 9 of 25\n",
      "[INFO] Epoch = 10 of 25\n",
      "[INFO] Epoch = 11 of 25\n",
      "[INFO] Epoch = 12 of 25\n",
      "[INFO] Epoch = 13 of 25\n",
      "[INFO] Epoch = 14 of 25\n",
      "[INFO] Epoch = 15 of 25\n",
      "[INFO] Epoch = 16 of 25\n",
      "[INFO] Epoch = 17 of 25\n",
      "[INFO] Epoch = 18 of 25\n",
      "[INFO] Epoch = 19 of 25\n",
      "[INFO] Epoch = 20 of 25\n",
      "[INFO] Epoch = 21 of 25\n",
      "[INFO] Epoch = 22 of 25\n",
      "[INFO] Epoch = 23 of 25\n",
      "[INFO] Epoch = 24 of 25\n",
      "[INFO] Epoch = 25 of 25\n",
      "[INFO] Neural network training is finished after 6.0101847648620605 seconds\n",
      "[INFO] Training set accuracy is 0.9903777777777778\n",
      "[INFO] Validation set accuracy is 0.9810666666666666\n"
     ]
    }
   ],
   "source": [
    "numberOfImages = 60000\n",
    "\n",
    "# The datapoints are in mnistData[0][0]\n",
    "X = data[0][0][:numberOfImages].reshape([numberOfImages,28*28]).astype('float')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Apply PCA to the data matrix X\n",
    "X = PCA(X, 0.8)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('[INFO] PCA finished in', end - start, 'seconds')\n",
    "\n",
    "# Normalize the data\n",
    "X = X/255.0\n",
    "\n",
    "# The labels are in mnistData[0][1]\n",
    "Y = data[0][1][:numberOfImages]\n",
    "\n",
    "# randomly choose 75% of the data to be the training set and 25% for the validation set\n",
    "(trainX, valX, trainY, valY) = train_test_split(X, Y, test_size = 0.25)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# fit the model to the training data\n",
    "print('[INFO] Now training the neural network')\n",
    "\n",
    "model = FeedforwardNeuralNetwork([X.shape[1], 128, 10], 0.001, ['exp', 0.1], 20.0, 300.0, 64, 0.9,\n",
    "                                 ['Glorot', 'uniform'], 'ReLU', 'cross-entropy')\n",
    "\n",
    "model.fit(trainX, trainY, valX, valY, 25, 1, False)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('[INFO] Neural network training is finished after', end - start, 'seconds')\n",
    "\n",
    "# Compute the outputs and measure the accuracy on the train and validation sets\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "predictedY = model.predict(trainX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "trainY = trainY.argmax(axis=1)\n",
    "print('[INFO] Training set accuracy is', accuracy_score(trainY, predictedY))\n",
    "\n",
    "valY = LabelBinarizer().fit_transform(valY)\n",
    "predictedY = model.predict(valX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "valY = valY.argmax(axis=1)\n",
    "print('[INFO] Validation set accuracy is', accuracy_score(valY, predictedY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, requiring a number of dimensions explaining 80\\% of the variance of the dataset with PCA reduced the dimension from 784 to 44, a reduction of over 94\\%! What is even more amazing is that the training accuracy on the lower-dimensional data is still 98\\%, but the time for PCA (4 seconds) + the time of training (6 seconds) is only 10 seconds, about 1/4 the time as without PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
